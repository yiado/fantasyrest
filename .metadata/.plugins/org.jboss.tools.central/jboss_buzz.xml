<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Testing with Awaitility made simple</title><link rel="alternate" href="http://www.mastertheboss.com/various-stuff/testing-java/testing-with-awaitility-made-simple/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/various-stuff/testing-java/testing-with-awaitility-made-simple/</id><updated>2022-08-26T15:44:54Z</updated><content type="html">The Awaitility library introduces a functional style usage to express expectations of an asynchronous system in a concise and easy to read manner. Let’s have a deep dive into it with this tutorial. Overview of Awaitility Awaitility (as the name says) checks expectations based on a timing condition. Here is a basic example of it: ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Deploying DashBuilder client on OpenShift Developer Sandbox</title><link rel="alternate" href="https://blog.kie.org/2022/08/deploying-dashbuilder-client-on-openshift-developer-sandbox.html" /><author><name>Manaswini Das</name></author><id>https://blog.kie.org/2022/08/deploying-dashbuilder-client-on-openshift-developer-sandbox.html</id><updated>2022-08-26T11:24:23Z</updated><content type="html">This blog post is going to walk you through building and running a custom Nginx Docker image for a sample DashBuilder static application to be deployed on OpenShift Developer Sandbox. DashBuilder has a client bundle, which consists of the DashBuilder Webapp. The deployment is simply modifying setup.js to include user dashboards and then serving index.html. In case you already have an image ready to deploy/publish on OpenShift Developer Sandbox, you can skip to the last section of this blog post. You can also use my image . CREATING A CUSTOM NGINX DOCKER IMAGE Normally, using Nginx with Docker, we would not Nginx from the official image and then configure it manually. We will use the pre-configured Nginx Docker Image and all we need to do is start the container from those Nginx Docker images and use them. In order to get started, we have to unpack an which contains the sample DashBuilder static application. Create a new folder and install the NPM package. mkdir newfolder cd newfolder npm i @kie-tools/dashbuilder-client Now we can get some sample YAML dashboards from and copy them into the dist folder inside node_modules/@kie-tools/dashbuilder-client. Now we also have to tweak setup.js to include the dashboards. Just add dashboards: [“sample.dash.yml”] inside the dashbuilder object inside setup.js. You can choose to add multiple dashboards too. Just add them as comma-separated strings in dashboards inside the dashbuilder object inside setup.js. In order to create an image, we need to know how an Nginx Docker image can be created. By default, Nginx looks in the /usr/share/nginx/html directory inside of the container for files to serve. We need to get our static files into this directory. One of the simplest ways to do this is to copy our HTML files into the image by building a custom image. To build a custom image, we’ll need to create a Dockerfile and add our commands to it. In the newfolder directory which now contains the node_modules folder, create a file named Dockerfile and paste the below commands. FROM nginxinc/nginx-unprivileged COPY /node_modules/@kie-tools/dashbuilder-client/dist/ /usr/share/nginx/html/ &gt; Note: We are using the since Openshift has limited permissions for Nginx and &gt; we may face errors like Openshift Nginx permission problem [nginx: [emerg] &gt; mkdir() “/var/cache/nginx/client_temp” failed (13: Permission denied)] while &gt; deploying our image to Openshift Developer Sandbox. So, it’s necessary to run &gt; Nginx as a non-root user. Refer to the “Running Nginx as a non-root user” &gt; section of for more information. We start building our custom image by using a base image. The FROM command will pull the nginxinc/nginx-unprivileged image to our local machine and then build our custom image on top of it. Next, we the contents of the folder containing the static content into the /usr/share/nginx/html directory inside the container overwriting the default index.html file provided by nginxinc/nginx-unprivileged image. You’ll notice that we did not add an ENTRYPOINT or a CMD to our Dockerfile. We will use the underlying ENTRYPOINT and CMD provided by the base Nginx image. Make sure you have in your system. Once you have Docker installed in your system, start Docker using the following command. You can also use for the same: sudo service docker start &gt; Troubleshooting &gt; &gt; Fresh install Docker cgroup mountpoint does not exist. Just run the following &gt; command or refer to : &gt; &gt; sudo mkdir /sys/fs/cgroup/systemd &gt; sudo mount -t cgroup -o none,name=systemd cgroup /sys/fs/cgroup/systemd We now need to run the following command on a terminal inside the newfolder directory. docker build -f Dockerfile -t dashbuilder . You will see the following on your terminal on the successful execution of the above statement. Note: Don’t forget to add the . at the end else you might get error like: ”docker build” requires exactly 1 argument. See ‘docker build — help’. Usage: docker build [OPTIONS] PATH | URL | - Build an image from a Dockerfile Time to run the image. To run the image, run the following command: docker run -it — rm -d -p 8080:80 — name web dashbuilder web is the container name and dashbuilder is the image name. Standard practice is to name your containers for the simple reason that it is easier to identify what is running in the container and what application or service it is associated with. Just like good naming conventions for variables in your code makes it simpler to read. So goes naming your containers. To name a container, we must pass the — name flag to the run command. Open your browser and navigate to to make sure our HTML page is being served correctly. This is what you will see on your browser. Served index.html containing sample dashboard SHIPPING YOUR IMAGE It’s time to publish the image on an image registry so that we can later deploy it in OpenShift Developer Sandbox. I’m using Docker Hub. You can and receive free unlimited public repositories. You can also use . Sharing our images on will help others on our team pull the images and run them locally. This is also a great way to share your application with others outside of your teams such as testers and business owners. To push your images to Docker’s repository run the docker tag and then the docker push commands. You will first need to log in with your Docker ID. $ docker login $ docker tag dashbuilder &lt;dockerid&gt;/dashbuilder $ docker push &lt;dockerid&gt;/dashbuilder Check whether you see the following output on your terminal on successful execution. You will be able to see your repository on your Docker Hub account. Docker Hub repositories page now contains your image DEPLOY IMAGE ON OPENSHIFT DEVELOPER SANDBOX Refer to to set up and start your OpenShift Developer Sandbox for free. Once you start using your sandbox, you can either use the CLI or the GUI to publish/deploy your image. In case you are comfortable with using CLI, you can refer to to deploy and expose your application. Alternatively, you can use the GUI to deploy your image. Click on the Hamburger button on the top left and click on the “+Add” button. You will see the following screen. Add menu in OpenShift Developer Sandbox Select “Container Images” and start filling the Deploy image form as follows. Page 1 of 2 of Deploy Image Page 2 of Deploy Image Now click on “Create”. This will immediately take you to the page corresponding to the Topology tab on your right menu. Topology view Click on the same and navigate to the “Resources” tab and you will be able to see a link in the “Routes” section. Click on the link and see your application running on another tab. Routes section in Resources tab DashBuilder static application running on OpenShift Developer Sandbox Great! You can now create an Nginx Docker image and deploy the image on OpenShift Developer Sandbox. Let us know whether you were able to get your DashBuilder static application to run successfully. The post appeared first on .</content><dc:creator>Manaswini Das</dc:creator></entry><entry><title type="html">Kogito 1.26.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/08/kogito-1-26-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/08/kogito-1-26-0-released.html</id><updated>2022-08-26T03:00:57Z</updated><content type="html">We are glad to announce that the Kogito 1.26.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Default value for enumerations in GRPC is included in response if property kogito.grpc.enum.includeDefault is set to true * Changing rejection handler policy for event executor thread. Rather than executing task in the same thread (which might lead to blocking exception), the emitter is stopped while the executor service is full * Avoiding creation of unneeded process instances when workflow json input schema validation fails.  For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.22.0 artifacts are available at the . A detailed changelog for 1.26.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Quarkus Tools for IntelliJ 1.12.0 released!</title><link rel="alternate" href="https://quarkus.io/blog/intellij-quarkus-tools-1.12.0/" /><author><name>Jeff Maury</name></author><id>https://quarkus.io/blog/intellij-quarkus-tools-1.12.0/</id><updated>2022-08-26T00:00:00Z</updated><content type="html">We are very pleased to announce the 1.12.0 release of Quarkus Tools for IntelliJ. This release improves the Quarkus wizard and Quarkus run experience but also aligns with LSP4MP 0.5.0 and quarkus-ls 0.12.1. Improved Quarkus wizard The Quarkus wizard can be used from File → New → Module → Quarkus....</content><dc:creator>Jeff Maury</dc:creator></entry><entry><title>Optimize loops with long variables in Java</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/25/optimize-loops-long-variables-java" /><author><name>Roland Westrelin</name></author><id>7ff3d707-eeb0-4375-b621-eb93c88316fa</id><updated>2022-08-25T07:00:00Z</updated><published>2022-08-25T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/06/23/how-jit-compiler-boosts-java-performance-openjdk"&gt;just-in-time (JIT) compiler&lt;/a&gt; in &lt;a href="https://openjdk.org"&gt;OpenJDK&lt;/a&gt; improves &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; performance through a number of optimizations, particularly in loops. Until recently, many optimizations worked only when the loop index was an &lt;code&gt;int&lt;/code&gt; variable. This article shows how the &lt;a href="https://openjdk.org/groups/hotspot"&gt;HotSpot virtual machine&lt;/a&gt; was upgraded to add the same optimizations for &lt;code&gt;long&lt;/code&gt; variables. The article covers particularly out-of-bounds checking (also called &lt;em&gt;range checks&lt;/em&gt;).&lt;/p&gt; &lt;h2&gt;Why optimization was added for long variables&lt;/h2&gt; &lt;p&gt;One of the important promises of Java, along with many other modern languages, is to catch out-of-bounds errors, such as when you mistakenly end a loop at &lt;code&gt;array.length&lt;/code&gt; instead of at &lt;code&gt;array.length-1&lt;/code&gt;. The HotSpot virtual machine eliminates range checks when possible as a performance optimization. As discussed in a &lt;a href="https://developers.redhat.com/articles/2022/03/16/range-check-elimination-loops-openjdks-hotspot-jvm"&gt;previous article&lt;/a&gt;, the JIT compiler enables range checks when the compiler is not sure that an index will stay within the bounds of an array.&lt;/p&gt; &lt;p&gt;The JIT compiler implements range checks as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;for (int i = start; i &lt; stop; i += stride)) { if (scale * i + offset &gt;=u array.length) { // range check deoptimize(); } // access to element scale * i + offset of array }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the previous code, &lt;code&gt;&gt;=u&lt;/code&gt; is an unsigned comparison, and the &lt;code&gt;deoptimize()&lt;/code&gt; function causes execution of the thread to continue in the interpreter where the out of bound exception is thrown.&lt;/p&gt; &lt;p&gt;Until recent versions of OpenJDK, optimization of range checks worked only if the loop variable &lt;code&gt;i&lt;/code&gt; was an &lt;code&gt;int&lt;/code&gt;. For a &lt;code&gt;long&lt;/code&gt;, range checks would always be performed. Many other optimizations were also unavailable.&lt;/p&gt; &lt;p&gt;The reason for restricting loop optimization to &lt;code&gt;int&lt;/code&gt; indexes was that they were considered the only ones common enough to deserve special treatment. One reason is that loops commonly iterate over arrays. Because the size of a Java array is a 32-bit integer, an &lt;code&gt;int&lt;/code&gt; is the natural choice for the loop variable.&lt;/p&gt; &lt;p&gt;Usages are evolving, though. The &lt;a href="https://openjdk.org/projects/panama/"&gt;Panama Project&lt;/a&gt; offers developers a better way to get access to off-heap memory areas. Offsets within memory are 64 bits, so loops that iterate over memory with that API tend to use a &lt;code&gt;long&lt;/code&gt; loop variable.&lt;/p&gt; &lt;p&gt;The lack of proper optimizations for &lt;code&gt;long&lt;/code&gt; counted loops initially was such a pain point for the Panama Project that it implemented workarounds in the library code to detect cases where the offsets fit into a 32-bit integer. In such cases, the project included special code that the JIT could better optimize. But once the improvements covered in this article were rolled out, those workarounds became unnecessary and could be removed. The end result is simplified library code with better overall performance.&lt;/p&gt; &lt;p&gt;The loop shape that needs to be optimized is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;for (long i = start; i &lt; stop; i += stride)) { if (scale * i + offset &gt;=u length) { // range check deoptimize(); } // access to memory at offset scale * i + offset }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The discussion in this article assumes a loop that goes upward (that is, an index that increases), along with a positive &lt;code&gt;scale&lt;/code&gt;. The project has generalized the optimization to accommodate a decreasing loop and a negative &lt;code&gt;scale&lt;/code&gt; as well, but we won't include those cases because they would overcomplicate the discussion.&lt;/p&gt; &lt;h2&gt;Recognizing and optimizing long counted loops&lt;/h2&gt; &lt;p&gt;Teaching the compiler to recognize the new loop shape with a &lt;code&gt;long&lt;/code&gt; loop variable instead of an &lt;code&gt;int&lt;/code&gt; loop variable is fairly straightforward, but is not sufficient to enable existing optimizations such as range check elimination, loop unrolling, and vectorization. Those optimization passes must be adapted to operate on a &lt;code&gt;long&lt;/code&gt; counted loop as well. Doing so is not a simple matter of including &lt;code&gt;long&lt;/code&gt; variables, because some of the optimizations have to deal with integer overflow. They were protected from overflow for &lt;code&gt;int&lt;/code&gt; variables by promoting some 32-bit integer values to 64-bit integers. Supporting these same optimizations on 64-bit integers would require promotion to the next larger integer type (probably 128 bits), for which the compiler lacks support.&lt;/p&gt; &lt;p&gt;Is there a way to upgrade the existing optimizations for a &lt;code&gt;long&lt;/code&gt; without having to rewrite these optimizations and incur a high risk of introducing bugs?&lt;/p&gt; &lt;p&gt;The solution we went with started by transforming the &lt;code&gt;long&lt;/code&gt; counted loop into a nested loop with an &lt;code&gt;int&lt;/code&gt; counted inner loop. The previous example was transformed roughly to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;for (long i = start; i &lt; stop;) { int j; for (j = 0; j &lt; min(stop - i, max_int); j += (int)stride) { if (scale * (j + i) + offset &gt;=u length) { // range check deoptimize(); } // access to memory at offset scale * (j+i) + offset } i += j; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, &lt;code&gt;max_int&lt;/code&gt; represents the largest signed 32-bit integer. The loop variable for the inner loop is also a 32-bit integer. That inner loop has the shape of a counted loop, so it's subject to existing counted loop optimizations such as unrolling. Adding an extra loop has some overhead, but if the original loop executes for a large number of iterations, most of the time should be spent in the inner loop and the cost of setting it up should be negligible.&lt;/p&gt; &lt;p&gt;Note that this transformation is valid only for &lt;code&gt;stride&lt;/code&gt; values that fit into a 32-bit integer, and pays off mostly if &lt;code&gt;stride&lt;/code&gt; is a relatively small 32-bit integer. (Otherwise, the inner loop executes only a small number of iterations and the overhead of nested loops is bigger.)&lt;/p&gt; &lt;p&gt;This extra transformation has the benefit of enabling several existing loop optimizations to &lt;code&gt;long&lt;/code&gt; loop counters. But one important optimization still isn't triggered by the transformed loop: Eliminating the range check. Indeed, the range check in the previous loop nest is still expressed in terms of 64-bit integers: &lt;code&gt;j+i&lt;/code&gt; and &lt;code&gt;length&lt;/code&gt;. The next section discusses how we make sure the range check has a proper shape so that the compiler recognizes it and enables range check optimization.&lt;/p&gt; &lt;h2&gt;A new API point for range checks&lt;/h2&gt; &lt;p&gt;A prerequisite for range check elimination is to make sure the loop has a shape that the compiler can properly optimize. In particular, the range check in the loop has to follow a canonical shape like the following, with an unsigned comparison and a deoptimization if the range check fails:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;if (scale * i + offset &gt;=u length) { deoptimize(); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The array accesses in Java include a range check, so the compiler is free to generate the pattern that works best for optimization. In the Panama Project's memory access API, the range check is not built into the language. So the check would have to be performed explicitly by the API in code similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;long o = scale * i + offset; if (o &gt;= length || o &lt; 0) { throw new SomeException(); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The JIT compiler would then have to recognize this code pattern as a range check, which would be complicated to perform reliably. After all, there would be more than one way to write this logic.&lt;/p&gt; &lt;p&gt;We have a more robust solution, which involves extending the core Java libraries with a new API point for range checks:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;long o = java.util.Objects.checkIndex(scale * i + offset, length);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;checkIndex()&lt;/code&gt; call already exists for &lt;code&gt;int&lt;/code&gt; arguments.&lt;/p&gt; &lt;p&gt;We have made the JIT compiler aware of the new API function. The compiler can use its own implementation for &lt;code&gt;checkIndex()&lt;/code&gt; as long as its behavior appears identical to the Java implementation to outside callers.&lt;/p&gt; &lt;p&gt;The technique of replacing a standard implementation of a function with one provided by the compiler is called a &lt;em&gt;compiler intrinsic&lt;/em&gt;. That implementation can be carefully crafted to allow optimizations.&lt;/p&gt; &lt;p&gt;The new &lt;code&gt;checkIndex()&lt;/code&gt; function and the corresponding underlying intrinsic are &lt;a href="https://github.com/openjdk/jdk/pull/1003"&gt;available as of JDK 16&lt;/a&gt;. Note that the new function is not restricted to the memory access API. It offers a reliable way of performing range checks that can be optimized well by the virtual machine, making the function a valuable addition to the core libraries for all developers.&lt;/p&gt; &lt;h2&gt;Optimizing long-range checks&lt;/h2&gt; &lt;p&gt;So far, we have discussed a transformation of the loop so that it becomes suitable for existing optimizations. In the same spirit, this section discusses how to transform range checks in order to trigger existing range check optimizations for &lt;code&gt;long&lt;/code&gt; indexes. We need to reshape the nested loop to:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;for (long i = start; i &lt; stop;) { int j; int scale' = ..; int offset' = ..; int length' = ..; for (j = 0; j &lt; min(stop - i, max_int); j += (int)stride) { if (scale' * j + offset' &gt;=u length') { deoptimize(); } // access to memory at offset scale * (j+i) + offset } i += j; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;scale'&lt;/code&gt;, &lt;code&gt;offset'&lt;/code&gt;, and &lt;code&gt;length'&lt;/code&gt; variables end with a prime symbol (&lt;code&gt;'&lt;/code&gt;) to denote a variable that is derived from or related to another variable. These variables are 32-bit integers that are invariant in the inner loop. Because the range check is expressed as a 32-bit comparison that operates on the loop variable of the inner loop, which is itself a loop with a 32-bit index, existing optimizations are triggered.&lt;/p&gt; &lt;p&gt;Assuming for instance that loop predication optimizes this loop nest, the result would be roughly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;for (long i = start; i &lt; stop;) { int j; int scale' = ..; int offset' = ..; int length' = ..; if (scale' * 0 + offset' &gt;=u length') { deoptimize(); } if (scale' * jmax + offset' &gt;=u length') { deoptimize(); } for (j = 0; j &lt; min(stop - i, max_int); j += (int)stride) { // access to memory at offset scale * (j+i) + offset } i += j; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;jmax&lt;/code&gt; is the largest value &lt;code&gt;j&lt;/code&gt; takes in the inner loop for a particular iteration &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Assuming, as we did before, that the inner loop runs for a large number of iterations, the range check becomes essentially free.&lt;/p&gt; &lt;p&gt;&lt;code&gt;scale'&lt;/code&gt;, &lt;code&gt;offset'&lt;/code&gt;, and &lt;code&gt;length'&lt;/code&gt; have to be derivatives of &lt;code&gt;scale&lt;/code&gt;, &lt;code&gt;offset&lt;/code&gt;, &lt;code&gt;length&lt;/code&gt;, and &lt;code&gt;i&lt;/code&gt;, the variables of the initial range check. They are all 64-bit integers. Let's see how to compute them.&lt;/p&gt; &lt;p&gt;&lt;code&gt;scale'&lt;/code&gt; can be set to &lt;code&gt;scale&lt;/code&gt;, but only if it fits in a 32-bit integer. Otherwise, there's no way to transform the range check. Another tricky issue here is that &lt;code&gt;scale' * j&lt;/code&gt; could overflow the &lt;code&gt;int&lt;/code&gt; range. One easy way around that problem is to adjust the inner loop's bounds so that overflow never happens, such as:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;for (long i = start; i &lt; stop;) { int j; int scale' = scale; int offset' = ..; int length' = ..; for (j = 0; j &lt; min(stop - i, max_int / scale); j += (int)stride) { if (scale' * j + offset' &gt;=u length') { deoptimize(); } // access to memory at offset scale * (j+i) + offset } i += j; }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If &lt;code&gt;scale&lt;/code&gt; turns out to be a relatively large 32-bit integer, the number of iterations of the inner loop is small and this transformation is unlikely to pay off. The Panama Project expects a small &lt;code&gt;scale&lt;/code&gt;. Typically, this variable is the size of some basic data type.&lt;/p&gt; &lt;p&gt;Let's note the range of values of &lt;code&gt;j&lt;/code&gt; for some iteration &lt;code&gt;i&lt;/code&gt; of the outer loop as&lt;code&gt;[0, jmax]&lt;/code&gt;. Then &lt;code&gt;scale * (i + j) + offset&lt;/code&gt; is in the range&lt;code&gt;[scale * i + offset, scale * (i + jmax) + offset]&lt;/code&gt; (remember that this discussion assumes a positive &lt;code&gt;scale&lt;/code&gt;). Let's call that interval &lt;code&gt;[range_min, range_max]&lt;/code&gt;. It's then possible to express &lt;code&gt;offset'&lt;/code&gt; and &lt;code&gt;length'&lt;/code&gt; in terms of &lt;code&gt;range_min&lt;/code&gt; and &lt;code&gt;range_max&lt;/code&gt;. In the simplest case (&lt;code&gt;range_min &gt;= 0&lt;/code&gt;, no overflow), we set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;offset' = 0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;length' = max(range_min, min(length, range_max+1)) - range_min&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's see why that works as expected—that is, why the transformed range check (expressed with &lt;code&gt;j&lt;/code&gt;) succeeds whenever the initial range check (expressed with &lt;code&gt;i&lt;/code&gt;) succeeds, and fails when the initial check fails.&lt;/p&gt; &lt;p&gt;We know that &lt;code&gt;j&lt;/code&gt; falls within &lt;code&gt;[0, jmax]&lt;/code&gt;. Then &lt;code&gt;range&lt;/code&gt; has the value &lt;code&gt;scale * (i + j) + offset&lt;/code&gt; and &lt;code&gt;range'&lt;/code&gt; has the value &lt;code&gt;scale * j + offset'&lt;/code&gt;. Using variables we defined earlier, &lt;code&gt;range&lt;/code&gt; is in &lt;code&gt;[range_min, range_max]&lt;/code&gt; and &lt;code&gt;range = range' + range_min&lt;/code&gt;. What happens for various values of &lt;code&gt;length&lt;/code&gt;?&lt;/p&gt; &lt;h3&gt;When length is greater than range_max&lt;/h3&gt; &lt;p&gt;In this case, the range check always succeeds. &lt;code&gt;length'&lt;/code&gt; is &lt;code&gt;range_max+1 - range_min&lt;/code&gt;. The range check becomes:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;range' &lt;u range_max+1 - range_min&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This expression is always true because &lt;code&gt;range&lt;/code&gt; is within &lt;code&gt;[range_min, range_max]&lt;/code&gt; and so &lt;code&gt;range'&lt;/code&gt; is within &lt;code&gt;[0, range_max - range_min]&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;When length is less than range_min&lt;/h3&gt; &lt;p&gt;In this case, the range check always fails. &lt;code&gt;length'&lt;/code&gt; is &lt;code&gt;0&lt;/code&gt;. The range check becomes &lt;code&gt;range'&lt;u 0&lt;/code&gt;, which is always false (no unsigned value can be negative).&lt;/p&gt; &lt;h3&gt;When length is somewhere in [range_min, range_max]&lt;/h3&gt; &lt;p&gt;If &lt;code&gt;length&lt;/code&gt; is in &lt;code&gt;[range_min, range_max]&lt;/code&gt;, the range check succeeds sometimes and fails other times. &lt;code&gt;length'&lt;/code&gt; is &lt;code&gt;length - range_min&lt;/code&gt;. The range check becomes:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;range' &lt;u length - range_min&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;which is equivalent to:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;0 &lt;= range' &lt; length - range_min &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt; &lt;code&gt;range_min &lt;= range' + range_min &lt; length &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt; &lt;code&gt;range_min &lt;= range &lt; length&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because we assumed &lt;code&gt;range_min&lt;/code&gt; to be positive, this is the same as &lt;code&gt;range &lt;u length&lt;/code&gt;, the range check before transformation.&lt;/p&gt; &lt;p&gt;&lt;code&gt;length'&lt;/code&gt; must be a 32-bit integer, but is computed in terms of 64-bit values. Note, however, that &lt;code&gt;range_max - range_min&lt;/code&gt; fits in a 32-bit integer, because loop bounds require &lt;code&gt;scale * j&lt;/code&gt; to fit in a 32-bit integer. That, in turn, guarantees that &lt;code&gt;length'&lt;/code&gt; can safely be stored as a 32-bit value.&lt;/p&gt; &lt;h2&gt;Extensive optimizations accommodate more Java loops&lt;/h2&gt; &lt;p&gt;This article covered recent optimizations in the OpenJDK HotSpot virtual machine that support loops with &lt;code&gt;long&lt;/code&gt; loop variables, with a particular focus on range checks operating on it. I gave an overview of the code patterns that developers can expect to be properly optimized. The article also demonstrated how new APIs require special virtual machine support and how the entire Java platform evolves to meet changing usages. Finally, I showed how reshaping a loop with a &lt;code&gt;long&lt;/code&gt; index enables a range of optimizations.&lt;/p&gt; &lt;p&gt;The cases we didn't cover in this article—loops going downward and a negative &lt;code&gt;scale&lt;/code&gt;—are discussed in a &lt;a href="https://github.com/openjdk/jdk/blob/ef266d77b6eb54d7e30a0aafd8a3e8c8f4f0e43a/src/hotspot/share/opto/loopnode.cpp#L1150"&gt;lengthy comment&lt;/a&gt; added to the &lt;a href="https://github.com/openjdk/jdk/pull/2045"&gt;JDK change request&lt;/a&gt;, augmented by &lt;a href="https://github.com/openjdk/jdk/pull/6989"&gt;this follow-up&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/25/optimize-loops-long-variables-java" title="Optimize loops with long variables in Java"&gt;Optimize loops with long variables in Java&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Roland Westrelin</dc:creator><dc:date>2022-08-25T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 25 August 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-08-25.html" /><category term="quarkus" /><category term="kubernetes" /><category term="java" /><category term="jakarta" /><category term="infinispan" /><category term="wildfly" /><category term="cloud-native" /><category term="openshift" /><category term="kogito" /><category term="drools" /><category term="keycloak" /><author><name>Pedro Silva</name><uri>https://www.jboss.org/people/pedro-silva</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-08-25.html</id><updated>2022-08-25T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kubernetes, java, jakarta, infinispan, wildfly, cloud-native, openshift, kogito, drools, keycloak"&gt; &lt;h1&gt;This Week in JBoss - 25 August 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone! It’s great to be back and bringing you another edition of the JBoss Editorial. As always there’s a lot of exciting news and updates from JBoss communities so let’s dive in.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-11-3-final-released/"&gt;Quarkus 2.11.3.Final released - Fix for CVE-2022-2466 and other bugfixes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_hola_你好_こんにちは_quarkus"&gt;Hola, 你好, こんにちは Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/l10n-of-quarkusio/"&gt;Hola, 你好, こんにちは Quarkus&lt;/a&gt;, by Max Rydahl Andersen&lt;/p&gt; &lt;p&gt;Since the launch of Japanese &lt;a href="http://ja.quarkus.io/"&gt;ja.quarkus.io&lt;/a&gt;, we got contributors translating the website into (simplified) Chinese at &lt;a href="http://cn.quarkus.io"&gt;cn.quarkus.io&lt;/a&gt; and recently we added Spanish at &lt;a href="http://es.quarkus.io"&gt;es.quarkus.io&lt;/a&gt; too. These sites are now accessible via the drop-down "globe" menu in the top-right corner.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_using_rhosak_from_wildfly"&gt;Using RHOSAK from WildFly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/08/19/RHOSAK/"&gt;Using RHOSAK from WildFly&lt;/a&gt;, by Kabir Khan&lt;/p&gt; &lt;p&gt;In this post Kabir shows how to write a simple application which sends and receives messages to/from a Kafka instance using &lt;strong&gt;RHOSAK&lt;/strong&gt;. RHOSAK - Red Hat OpenShift Streams for Apache Kafka, is a cloud service hosted by Red Hat which makes setting up, managing, and scaling Apache Kafka instances very easy&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_getting_started_with_atlasmap"&gt;Getting started with AtlasMap&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/java/getting-started-with-atlasmap/"&gt;Getting started with AtlasMap&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;In this article, Francesco shows the basics of the AtlasMap data mapping solution. He also covers the basic set up of the Web UI and how to use it to create a minimal mapping file. Finally, he discusses two simple ways to use AtlasMap in your Java projects.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_optimize_loops_with_long_variables_in_java"&gt;Optimize loops with long variables in Java&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/08/25/optimize-loops-long-variables-java#why_optimization_was_added_for_long_variables"&gt;Optimize loops with long variables in Java&lt;/a&gt;, by Roland Westrelin&lt;/p&gt; &lt;p&gt;The just-in-time (JIT) compiler in OpenJDK improves Java performance through a number of optimizations, particularly in loops. Until recently, many optimizations worked only when the loop index was an int variable. In this article, Roland shows how the HotSpot virtual machine was upgraded to add the same optimizations for long variables.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_a_dmn_feel_handbook"&gt;A DMN FEEL HANDBOOK&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/08/a-dmn-feel-handbook.html"&gt;A DMN FEEL handbook&lt;/a&gt;, by Matteo Mortari&lt;/p&gt; &lt;p&gt;In this announcement, Matteo introduces an (experimental) DMN FEEL handbook, a helpful companion for your DMN modeling activities! You can access the FEEL handbook at &lt;a href="https://kiegroup.github.io/dmn-feel-handbook/#dmn-feel-handbook"&gt;DMN FEEL handbook&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;From unmissable demos to brilliant chat about the latest Java trends, the JBoss community has some great video content for you:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=lLyVDqVK8cE"&gt;Quarkus Insights #99: Using Quarkus CodeStarts&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=7Q34Za79g18"&gt;Kubernetes Master Class - Avoiding configuration drift with Argo CD&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_see_you_next_time"&gt;See you next time&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;em&gt;Hope you enjoyed this edition. Please join us again in two weeks for our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/pedro-silva.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Pedro Silva&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Pedro Silva</dc:creator></entry><entry><title>Move from apt to dnf package management</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/24/move-apt-dnf-package-management" /><author><name>Bob Reselman</name></author><id>7989058e-2a55-483d-926e-36cbd5eff3bf</id><updated>2022-08-24T07:00:00Z</updated><published>2022-08-24T07:00:00Z</published><summary type="html">&lt;p&gt;A package manager makes it simple to install &lt;a href="https://developers.redhat.com/topics/linux"&gt;GNU/Linux&lt;/a&gt; applications on a local computer. Before package management became commonplace, installing applications was a tedious, error-prone undertaking. The ease a package manager brings to installing an application on a Linux computer has been a major factor contributing to the widespread adoption of Linux as a mainstream operating system for both business and home users.&lt;/p&gt; &lt;p&gt;Package management under Linux is divided, however. Two major systems co-exist:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The APT package management system for Debian and its many derivatives, notably Ubuntu. Packages are marked with the &lt;code&gt;.deb&lt;/code&gt; suffix and are managed through the &lt;code&gt;apt&lt;/code&gt; command-line interface (CLI).&lt;/li&gt; &lt;li&gt;The RPM package management system for &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; and its derivatives, notably Fedora and CentOS Stream. Packages are marked with the &lt;code&gt;.rpm&lt;/code&gt; suffix and are managed through the &lt;code&gt;dnf&lt;/code&gt; CLI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article is for developers who are currently using Debian-based systems and are familiar with APT, but want to start using a system in the Red Hat Enterprise Linux family. The article explains the similarities and differences between APT and RPM. I show how to execute specific, commonplace package management tasks using each system.&lt;/p&gt; &lt;h2&gt;Understanding apt, dnf, and yum&lt;/h2&gt; &lt;p&gt;Debian users are accustomed to managing their packages via the &lt;code&gt;apt&lt;/code&gt; command. Switching to the current RPM tool, &lt;code&gt;dnf&lt;/code&gt;, is the topic of this article.&lt;/p&gt; &lt;p&gt;You might also have seen references to a &lt;code&gt;yum&lt;/code&gt; command. Both &lt;code&gt;dnf&lt;/code&gt; and &lt;code&gt;yum&lt;/code&gt; are command-line utilities that work with RPM packages. Red Hat originally released and depended on &lt;code&gt;yum&lt;/code&gt;, which is an acronym for &lt;em&gt;Yellowdog Updater, Modified&lt;/em&gt;. &lt;code&gt;dnf&lt;/code&gt;, an abbreviation for &lt;em&gt;dandified yum&lt;/em&gt;, is the follow-up technology—based on &lt;code&gt;yum&lt;/code&gt;, as the name implies.&lt;/p&gt; &lt;p&gt;Today, &lt;code&gt;dnf&lt;/code&gt; is the default package management utility for Red Hat Enterprise Linux, Fedora, and CentOS Stream, and has been so since Fedora 22, CentOS 8, and Red Hat Enterprise Linux 8, respectively. &lt;code&gt;yum&lt;/code&gt; has been deprecated as the default package manager in the Red Hat family of distributions, so while &lt;code&gt;yum&lt;/code&gt; commands currently work, it's best to use just &lt;code&gt;dnf&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Understanding package discovery and installation&lt;/h2&gt; &lt;p&gt;The pattern for finding and installing a Linux package is essentially the same whether you're using &lt;code&gt;apt&lt;/code&gt; or &lt;code&gt;dnf&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;When you execute a command to install a package, the package manager looks at configuration files on the local machine to determine the location of a repository that has a given package on the internet. Then the installation command downloads the package along with its dependencies from the internet. Finally, the package manager installs and configures the application on the local machine. Figure 1 illustrates the process.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/patt.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/patt.png?itok=RGemPlHO" width="1111" height="602" alt="A package manager gets information from the local machine to retrieve a package from a repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A package manager gets information from the local machine to retrieve a package from a repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Although the basic installation process is similar for both the RPM and APT package managers, there are distinctions when it comes to implementation. Besides the commands used, there's a bit of difference in the way these commands consult files to find and install packages.&lt;/p&gt; &lt;p&gt;When you invoke a package manager's installation command, the package manager first looks to see whether the package of interest is present and already installed. APT looks in the &lt;code&gt;/var/cache/apt/archives&lt;/code&gt; directory for the presence of the package's &lt;code&gt;.deb&lt;/code&gt; file. Under RPM, the package manager inspects the directories in &lt;code&gt;/var/cache/dnf/&lt;/code&gt; for a package's &lt;code&gt;.rpm&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;If the package's &lt;code&gt;.deb&lt;/code&gt; or &lt;code&gt;.rpm&lt;/code&gt; file is not present, the installer inspects files that describe the locations of repositories on the internet. In some cases, these files can report the packages that are stored in a particular repository as well. Under APT, the information is stored in the file &lt;code&gt;/etc/apt/sources.list &lt;/code&gt;or in &lt;code&gt;.list&lt;/code&gt; files in the directory &lt;code&gt;/etc/apt/sources.list.d&lt;/code&gt;. Under RPM, repository details are stored in &lt;code&gt;.xml&lt;/code&gt; files or compressed &lt;code&gt;.solvx&lt;/code&gt; files in the cache directory &lt;code&gt;/var/cache/dnf/&lt;/code&gt;. Also, general information about a repository is stored in &lt;code&gt;.repo&lt;/code&gt; files in the directory &lt;code&gt;/etc/yum.repos.d&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After discovering information about a package and repository, the installer goes to a remote repository to find and install the package. In some cases, the installer might have to go to a number of remote repositories looking for a package. Should the installer not find a package of interest, the package manager reports an error.&lt;/p&gt; &lt;p&gt;Thus, the overall pattern for package discovery and installation for APT and RPM is similar. The difference is in the CLI tools used. The structure and contents of the file system used by the package installer on the local computer differ as well.&lt;/p&gt; &lt;p&gt;Most users migrating from Debian's &lt;code&gt;apt&lt;/code&gt; to RPM's &lt;code&gt;dnf&lt;/code&gt; never have to concern themselves with the difference between the internals of the APT and RPM package managers. Low-level operations have been abstracted away by the CLI tools.&lt;/p&gt; &lt;h2&gt;Executing commonplace commands&lt;/h2&gt; &lt;p&gt;In most cases, the main concern of developers migrating from &lt;code&gt;apt&lt;/code&gt; to &lt;code&gt;dnf&lt;/code&gt; is installing, removing, and updating packages, which we'll cover in this section. But first, I'll show you how to add a repository to search and how to list packages and known repositories.&lt;/p&gt; &lt;h3&gt;Adding a repository for the package manager to search&lt;/h3&gt; &lt;p&gt;To install a package on your computer, the CLI tool needs to know where the repositories containing the packages are. Typically, when you first set up your computer, whether it's running an operating system from the Debian family or the Red Hat Enterprise Linux family, information about the basic repositories that host the usual packages for the given operating system is included in the OS by default. However, there might be times when you need to search other repositories. This section shows the commands for adding information about a repository to a local computer.&lt;/p&gt; &lt;p&gt;To add information about a repository to a computer running Debian, Ubuntu, etc., enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# add-apt-repository &lt;repository identification information&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, the following command installs information for the MongoDB database on a  Debian machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo add-apt-repository 'deb [arch=amd64] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.0 multiverse'&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Commands that change system software must be entered as the superuser (root). Hence, these commands are prefixed with &lt;code&gt;sudo&lt;/code&gt;. I show the command prompt &lt;code&gt;#&lt;/code&gt; as a reminder that you must be running as root.&lt;/p&gt; &lt;p&gt;To add information about a repository to a computer running Red Hat Enterprise Linux, Fedora, or CentOS Stream, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# dnf config-manager --add-repo &lt;repo_url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example installs the information about a mirror repository for CentOS 9 on the local machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf config-manager --add-repo="https://mirror.aarnet.edu.au/pub/centos/9"&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Updating the repository&lt;/h3&gt; &lt;p&gt;You also want to routinely make sure that current packages on the host computer are up to date. To update existing packages on Debian, run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo apt update&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To update existing packages on a computer in the Red Hat Enterprise Linux family, execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf update&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Installing an application&lt;/h3&gt; &lt;p&gt;As mentioned previously, the pattern for installing packages on a host computer is similar in Debian and machines in the Red Hat Enterprise Linux family.&lt;/p&gt; &lt;p&gt;To install a package on a Debian machine, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo apt install &lt;package_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example installs the &lt;code&gt;jq&lt;/code&gt; utility for parsing and filtering JSON files on a Debian machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo apt install jq&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To install a package on a machine in the Red Hat Enterprise Linux family, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf install &lt;package_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example installs the &lt;code&gt;jq&lt;/code&gt; utility on a machine in the Red Hat Enterprise Linux family:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf install jq&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Removing an application&lt;/h3&gt; &lt;p&gt;The pattern for removing an application is similar for systems based on Debian and on Red Hat Enterprise Linux. The difference is the CLI tool used.&lt;/p&gt; &lt;p&gt;To remove a package on a Debian machine, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo apt remove &lt;package_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example removes the &lt;code&gt;jq&lt;/code&gt; utility from a Debian machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo apt remove jq&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To remove a package on Red Hat Enterprise Linux, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf remove &lt;package_name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example removes the &lt;code&gt;jq&lt;/code&gt; utility from a Red Hat Enterprise Linux machine:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo dnf remove jq&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Getting a list of packages installed on a host computer&lt;/h3&gt; &lt;p&gt;Listing the packages installed on a local machine can furnish useful information, particularly for auditing and system management.&lt;/p&gt; &lt;p&gt;To list all the packages installed on a machine running Debian, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ apt list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example shows you how to use that command in conjunction with the &lt;code&gt;grep&lt;/code&gt; command to filter the results using a regular expression. The regular expression in this example saves only the lines that start with the characters &lt;code&gt;git&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ apt list | grep '^git'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following output shows a partial list of the results:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;git-annex-remote-rclone/focal,focal 0.6-1 all git-annex/focal 8.20200226-1 amd64 git-build-recipe/focal,focal 0.3.6 all git-buildpackage-rpm/focal,focal 0.9.19 all git-buildpackage/focal,focal 0.9.19 all git-cola/focal,focal 3.6-1 all git-crecord/focal,focal 20190217~git-1 all . . .&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To list the packages installed on a Red Hat Enterprise Linux machine, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf list installed&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following example shows how to use that command to get a list of packages installed on a Red Hat Enterprise Linux machine and then pick out lines that start with &lt;code&gt;git&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf list installed | grep '^git' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command produces the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;git.x86_64 2.35.3-1.fc35 @updates git-core.x86_64 2.35.3-1.fc35 @updates git-core-doc.noarch 2.35.3-1.fc35 @updates&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Getting a list of repositories known to a host computer&lt;/h3&gt; &lt;p&gt;Debian and the Red Hat Enterprise Linux family have different methods for listing repositories known to a given local computer.&lt;/p&gt; &lt;p&gt;Under a default installation of Debian, no single command has the logic to report known repositories. Instead, you have to finesse existing commands.&lt;/p&gt; &lt;p&gt;One way to list known repositories is to use the &lt;code&gt;apt-cache policy&lt;/code&gt; command to return the known repositories, as shown in the following example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ apt-cache policy |grep http |awk '{print $2 " " $3}' |sort -u&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We have seen &lt;code&gt;grep&lt;/code&gt; already. The &lt;code&gt;awk&lt;/code&gt; command that follows in the pipeline selects the second and third words of each line. The full command produces the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;http://dl.google.com/linux/chrome/deb stable/main http://ppa.launchpad.net/ansible/ansible/ubuntu focal/main http://security.ubuntu.com/ubuntu focal-security/main http://security.ubuntu.com/ubuntu focal-security/multiverse http://security.ubuntu.com/ubuntu focal-security/restricted http://security.ubuntu.com/ubuntu focal-security/universe http://us.archive.ubuntu.com/ubuntu focal-backports/main http://us.archive.ubuntu.com/ubuntu focal-backports/universe http://us.archive.ubuntu.com/ubuntu focal/main http://us.archive.ubuntu.com/ubuntu focal/multiverse http://us.archive.ubuntu.com/ubuntu focal/restricted http://us.archive.ubuntu.com/ubuntu focal/universe http://us.archive.ubuntu.com/ubuntu focal-updates/main http://us.archive.ubuntu.com/ubuntu focal-updates/multiverse http://us.archive.ubuntu.com/ubuntu focal-updates/restricted http://us.archive.ubuntu.com/ubuntu focal-updates/universe&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Another way to get a list of repositories is to inspect the entries that start with the characters &lt;code&gt;deb&lt;/code&gt; in the &lt;code&gt;etc/apt/sources.list&lt;/code&gt; files:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;# sudo grep -rhE ^deb /etc/apt/sources.list*&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command produces the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;deb http://us.archive.ubuntu.com/ubuntu/ focal main restricted deb http://us.archive.ubuntu.com/ubuntu/ focal-updates main restricted deb http://us.archive.ubuntu.com/ubuntu/ focal universe deb http://us.archive.ubuntu.com/ubuntu/ focal-updates universe deb http://us.archive.ubuntu.com/ubuntu/ focal multiverse&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Things are easier when using &lt;code&gt;dnf&lt;/code&gt; in the Red Hat Enterprise Linux family. The &lt;code&gt;dnf repolist&lt;/code&gt; command lists the repositories known to the local machine.&lt;/p&gt; &lt;p&gt;The following example shows the result of running the &lt;code&gt;dnf repolist&lt;/code&gt; command. By default the command displays the two columns, &lt;code&gt;repo id&lt;/code&gt; and &lt;code&gt;repo name&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dnf repolist&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command produces the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;repo id repo name fedora Fedora 35 - x86_64 fedora-cisco-openh264 Fedora 35 openh264 (From Cisco) - x86_64 fedora-modular Fedora Modular 35 - x86_64 updates Fedora 35 - x86_64 - Updates updates-modular Fedora Modular 35 - x86_64 - Updates&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;A powerful and simple package management system&lt;/h2&gt; &lt;p&gt;Moving from Debian or Ubuntu to Red Hat Enterprise Linux, Fedora, or CentOS Stream requires some adjustment when it comes to working at the command line, but the transition can be easy.&lt;/p&gt; &lt;p&gt;The patterns for installing and removing applications are surprisingly similar, yet the command line tools are different. Ubuntu/Debian uses &lt;code&gt;apt&lt;/code&gt;. The Red Hat Enterprise Linux family uses &lt;code&gt;dnf&lt;/code&gt;. Both command line tools support similar subcommands, which is most evident with &lt;code&gt;apt install&lt;/code&gt; and &lt;code&gt;dnf install&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The techniques for keeping track of known repositories differ between operating systems. Fortunately for those moving to the Red Hat Enterprise Linux family, listing repositories is a lot easier because it involves only the &lt;code&gt;dnf repolist&lt;/code&gt; command. Listing repositories under Debian requires more work.&lt;/p&gt; &lt;p&gt;Learning the details of a new technology takes time. When transitioning from &lt;code&gt;apt&lt;/code&gt; to &lt;code&gt;dnf&lt;/code&gt;, you'll have to anticipate a learning curve. But the learning curve is not steep and you'll be up and running in no time.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/24/move-apt-dnf-package-management" title="Move from apt to dnf package management"&gt;Move from apt to dnf package management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-08-24T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.11.3.Final released - Fix for CVE-2022-2466 and other bugfixes</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-11-3-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-11-3-final-released/</id><updated>2022-08-24T00:00:00Z</updated><content type="html">2.11.3.Final is our third maintenance release for the 2.11 release train. It is a safe upgrade for anyone using 2.11. Among other issues, it fixes CVE-2022-2466 that was affected our quarkus-smallrye-graphql extension. Migration Guide If you are not already using 2.11, please refer to our migration guide. Full changelog You...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>How to use OpenTelemetry to trace Node.js applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/23/how-use-opentelemetry-trace-nodejs-applications" /><author><name>Annapurna Patil, Helio Frota, Rashmi Panchamukhi</name></author><id>090eca54-e9bd-4533-b223-1db3ca7060a3</id><updated>2022-08-23T07:00:00Z</updated><published>2022-08-23T07:00:00Z</published><summary type="html">&lt;p&gt;One great thing about &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; is how well it performs inside a container. The shift to &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized deployments and environments&lt;/a&gt; comes with extra complexity. This article addresses the added complexity of observability—seeing what's going on within your application and its resources. We will also cover how to set up &lt;a href="https://opentelemetry.io/"&gt;OpenTelemetry&lt;/a&gt; to achieve this visibility. This is useful when resource usage wanders outside of the expected norms.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://www.cncf.io"&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF) maintains a set of open source libraries and tools for visibility. OpenTelemetry is gaining momentum with developers to increase the observability of their Node.js applications through cross-component traces. OpenTelemetry with &lt;a href="https://www.jaegertracing.io/"&gt;Jaeger&lt;/a&gt; as a backend is a great option for tracing Node.js applications running inside of a container. Although OpenTelemetry is still in an incubated status at the CNCF, it is the leading choice for tracing. You can read more about why we believe in the importance of distributed tracing on the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/distributed-tracing.md"&gt;distributed tracing Node.js Reference Architecture page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article demonstrates a scenario that illustrates how the lack of integration tests can lead to the appearance of an error in production. We investigate the error on a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; platform using OpenTelemetry traces to quickly answer the following questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Where is the problem?&lt;/li&gt; &lt;li&gt;What is causing the error?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Follow this 5-step demonstration to troubleshoot production errors:&lt;/p&gt; &lt;h2&gt;Step 1.  Set up prerequisites&lt;/h2&gt; &lt;p&gt;Following the steps in this article, requires an OpenShift cluster with the &lt;a href="https://docs.openshift.com/container-platform/4.10/distr_tracing/distr_tracing_install/distr-tracing-deploying-jaeger.html"&gt;OpenShift distributed tracing platform Operator&lt;/a&gt; and &lt;a href="https://docs.openshift.com/container-platform/4.10/distr_tracing/distr_tracing_install/distr-tracing-deploying-otel.html"&gt;OpenShift distributed tracing data collection Operator (Technology Preview)&lt;/a&gt; installed.&lt;/p&gt; &lt;p&gt;We are using &lt;a href="https://developers.redhat.com/products/openshift-local/overview"&gt;OpenShift Local&lt;/a&gt; (formerly called Red Hat CodeReady Containers), which allows us to run a single-node OpenShift cluster locally. It doesn't have all the features of an OpenShift cluster. But OpenShift Local has everything we need for this article, and it's a good way to get started with OpenShift.&lt;/p&gt; &lt;p&gt;If you are going to use OpenShift Local, you can log in as &lt;code&gt;kubeadmin&lt;/code&gt; and install the Operators via &lt;a href="https://docs.openshift.com/container-platform/4.10/distr_tracing/distr_tracing_install/distr-tracing-installing.html"&gt;OperatorHub&lt;/a&gt; (Figure 1). If you work on an OpenShift cluster set up by an organization, ask the cluster administrator to install the Operators.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-image1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-image1.png?itok=YloMk8qm" width="600" height="252" alt="A screenshot of the OperatorHub page" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The OpenShift distributed tracing data collection Operator can be installed from the OperatorHub page shown in this screenshot. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Step 2. Run the CRUD application example&lt;/h2&gt; &lt;p&gt;For this demonstration, we will use the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;Nodeshift RESTful HTTP CRUD starter application&lt;/a&gt;. Clone this GitHub repository from the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/nodeshift-starters/nodejs-rest-http-crud.git&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate to the &lt;code&gt;nodejs-rest-http-crud&lt;/code&gt; directory of the cloned repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd nodejs-rest-http-crud&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Make sure you are logged into your OpenShift cluster as a developer, using &lt;code&gt;oc login&lt;/code&gt;. Create a new project called &lt;code&gt;opentel&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project opentel&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;nodejs-rest-http-crud&lt;/code&gt; example requires a PostgreSQL database. So install a Postgres db into your OpenShift cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app -e POSTGRESQL_USER=luke -e POSTGRESQL_PASSWORD=secret -e POSTGRESQL_DATABASE=my_data centos/postgresql-10-centos7 --name=my-database&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 3. Set up the Node.js program for tracing&lt;/h2&gt; &lt;p&gt;We are going to add a bug deliberately to the Node.js program so you can simulate the process of tracing a problem. Open the &lt;code&gt;lib/api/fruits.js&lt;/code&gt; file and change the SQL statement in the &lt;code&gt;create&lt;/code&gt; function from &lt;code&gt;INSERT INTO products&lt;/code&gt; to &lt;code&gt;INSERT INTO product0&lt;/code&gt;. Changing the last character to zero makes the statement query a nonexistent database table.&lt;/p&gt; &lt;p&gt;Now deploy the example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ npm run openshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once it's deployed, you should see the application and the database running in the developer topology view (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-Fig2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-Fig2.png?itok=_fPgYBJX" width="600" height="373" alt="The developer topology view of the application and the database." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The topology view shows two circles, one for the Node.js application and one for the database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The application exposes an endpoint that you can find by selecting the application and scrolling down to the &lt;strong&gt;Routes&lt;/strong&gt; section (Figure 3).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig3.png?itok=HtJ3878u" width="600" height="296" alt="A red arrow pointing to the endpoint under the routes section." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The route that allows access to the application can be found by clicking on the application in the topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;However, if you go to that page and try to add a fruit, the operation will fail and trigger a notification alert (see Figure 4). This error alert appears because the application has a typo inserted on the database table name. It should be &lt;code&gt;products&lt;/code&gt; instead of &lt;code&gt;product0&lt;/code&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig4.png?itok=x15v-Gyi" width="600" height="345" alt="A screenshot of an invalid SQL statement alert." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: An alert appears in the UI when an invalid SQL statement is issued. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Check the &lt;code&gt;lib/api/fruits.js&lt;/code&gt; file within the project you cloned. If you are using an IDE, note that the spell check cannot highlight the error (Figure 5).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig5.png?itok=UhDh4tyJ" width="600" height="58" alt="IDE does not flag a character 0 error in the code shown." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: There's an error in the code, but the IDE does not flag this particular error with a character 0. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;In other situations, the IDE will highlight a misspelled word (shown in Figure 6).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig6.png?itok=ElGKKnID" width="600" height="61" alt="The IDE highlights a spelling error in the code." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The IDE highlights an error such as an extraneous letter in the table name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The typo we introduced would likely have been caught by integration tests. But the problem preventing the program from running is an example of something that can happen in production resulting from a lack of test coverage. In cases like these, tracing can not only identify the component where the error occurred but also identify the exact problem.&lt;/p&gt; &lt;h2&gt;Step 4. Instrument the production application&lt;/h2&gt; &lt;p&gt;Now you can instrument your application to quickly identify what is happening. Normally you would already have your production application instrumented, but we are demonstrating this example step by step.&lt;/p&gt; &lt;p&gt;To instrument the application:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Add a number of OpenTelemetry dependencies to the &lt;code&gt;package.json&lt;/code&gt; file.&lt;/li&gt; &lt;li&gt;Create a file named &lt;code&gt;tracer.js&lt;/code&gt;  that will inject OpenTelemetry into the application.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;We will detail these two tasks in the following subsections:&lt;/p&gt; &lt;h3&gt;Add OpenTelemetry dependencies&lt;/h3&gt; &lt;p&gt;The following list shows the dependencies we added. You may want to use newer versions, depending on when you are reading this article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"@opentelemetry/api": "^1.1.0", "@opentelemetry/exporter-jaeger": "^1.3.1", "@opentelemetry/exporter-trace-otlp-http": "^0.29.2", "@opentelemetry/instrumentation": "^0.29.2", "@opentelemetry/instrumentation-express": "^0.30.0", "@opentelemetry/instrumentation-http": "^0.29.2", "@opentelemetry/instrumentation-pg": "^0.30.0", "@opentelemetry/resources": "^1.3.1", "@opentelemetry/sdk-node": "^0.29.2", "@opentelemetry/sdk-trace-base": "^1.3.1", "@opentelemetry/sdk-trace-node": "^1.3.1", "@opentelemetry/semantic-conventions": "^1.3.1", &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create the tracer.js file&lt;/h3&gt; &lt;p&gt;The content of the &lt;code&gt;tracer.js&lt;/code&gt; file is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;'use strict'; const { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api'); // SDK const opentelemetry = require('@opentelemetry/sdk-node'); // Express, postgres and http instrumentation const { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node'); const { registerInstrumentations } = require('@opentelemetry/instrumentation'); const { HttpInstrumentation } = require('@opentelemetry/instrumentation-http'); const { ExpressInstrumentation } = require('@opentelemetry/instrumentation-express'); const { PgInstrumentation } = require('@opentelemetry/instrumentation-pg'); // Collector trace exporter const { Resource } = require('@opentelemetry/resources'); const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions'); const { SimpleSpanProcessor } = require('@opentelemetry/sdk-trace-base'); const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http'); diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG); // Tracer provider const provider = new NodeTracerProvider({ resource: new Resource({ [SemanticResourceAttributes.SERVICE_NAME]: 'fruits' }) }); registerInstrumentations({ instrumentations: [ // Currently to be able to have auto-instrumentation for express // We need the auto-instrumentation for HTTP. new HttpInstrumentation(), new ExpressInstrumentation(), new PgInstrumentation() ] }); // Tracer exporter const traceExporter = new OTLPTraceExporter({ url: 'http://opentel-collector-headless.opentel.svc:4318/v1/traces' }); provider.addSpanProcessor(new SimpleSpanProcessor(traceExporter)); provider.register(); // SDK configuration and start up const sdk = new opentelemetry.NodeSDK({ traceExporter }); (async () =&gt; { try { await sdk.start(); console.log('Tracing started.'); } catch (error) { console.error(error); } })(); // For local development to stop the tracing using Control+c process.on('SIGINT', async () =&gt; { try { await sdk.shutdown(); console.log('Tracing finished.'); } catch (error) { console.error(error); } finally { process.exit(0); } }); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Don't worry, you don't need to change the core business code to make it work. You would just require &lt;code&gt;tracer.js&lt;/code&gt; at the top of the &lt;code&gt;app.js&lt;/code&gt; file. But we have already coded that line here. Now you only need to uncomment the &lt;code&gt;require('./tracer');&lt;/code&gt; line in our example.&lt;/p&gt; &lt;p&gt;This &lt;code&gt;tracer.js&lt;/code&gt; file is composed of several parts that refer to the plugins we are using. You could adapt the file for your specific needs. The following documentation provides more information:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/exporter-trace-otlp-http"&gt;OpenTelemetry Collector Exporter for web and node&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/instrumentation-pg"&gt;OpenTelemetry Postgres Instrumentation for Node.js&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/instrumentation-express"&gt;OpenTelemetry Express Instrumentation for Node.js&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/instrumentation-http"&gt;OpenTelemetry HTTP and HTTPS Instrumentation for Node.js&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/semantic-conventions"&gt;OpenTelemetry Semantic Conventions&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/@opentelemetry/sdk-trace-node"&gt;OpenTelemetry Node SDK&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Step 5. Trace with OpenTelemetry&lt;/h2&gt; &lt;p&gt;In this section, we will debug OpenTelemetry. This helps us troubleshoot our &lt;code&gt;tracer.js&lt;/code&gt; code.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Set up the trace as follows:&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;pre&gt; &lt;code&gt;const { diag, DiagConsoleLogger, DiagLogLevel } = require('@opentelemetry/api'); diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.DEBUG);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then create a new resource for &lt;code&gt;NodeTracerProvider&lt;/code&gt; to help identify our service inside Jaeger. In this case, we use the service name, &lt;code&gt;fruits&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;const provider = new NodeTracerProvider({ resource: new Resource({ [SemanticResourceAttributes.SERVICE_NAME]: 'fruits' }) });&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because this is an &lt;a href="https://www.npmjs.com/package/express"&gt;Express&lt;/a&gt; application that also uses PostgreSQL, we want to trace those layers. We also need to register &lt;code&gt;HttpInstrumentation&lt;/code&gt; to make &lt;code&gt;ExpressInstrumentation&lt;/code&gt; work.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Register the instrumentation:&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;pre&gt; &lt;code&gt;registerInstrumentations({ instrumentations: [ new HttpInstrumentation(), new ExpressInstrumentation(), new PgInstrumentation() ] });&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Create the following trace exporter:&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;pre&gt; &lt;code&gt;const traceExporter = new OTLPTraceExporter({ url: 'http://opentel-collector-headless.opentel.svc:4318/v1/traces' });&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can add an environment variable if you need to specify a different URL for the OpenTelemetry Collector.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Install Jaeger and OpenTelemetry Collector&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To continue this configuration, we need to give user admin rights on the &lt;code&gt;opentel&lt;/code&gt; project to the developer to successfully install both the Jaeger and OpenTelemetry Collector Operators.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc policy add-role-to-user admin developer -n opentel&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create and apply a Jaeger custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f tracing/jaeger.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create an OpenTelemetryCollector custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -f tracing/opentel-collector.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inside OpenShift, the topology menu now shows the components (Figure 7).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig7.png?itok=dHq75w87" width="600" height="331" alt="OpenShift topology menu showing OpenTelemetry and Jaeger components." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: OpenTelemetry and Jaeger components appear in the topology view. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;We have installed the Collector along with the auto-instrumentation plugins installed when we added &lt;code&gt;tracer.js&lt;/code&gt; to &lt;code&gt;app.js&lt;/code&gt;. Now, these plugins will catch and send the traces to the Collector instance in our namespace. The Collector will receive, process, and export them to the Jaeger instance in our namespace.&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;h3&gt;Check the traces:&lt;/h3&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Go back to the application and try to add a new fruit again. You will still get the same error, but traces of additional information now appear in the Jaeger UI.&lt;/p&gt; &lt;p&gt;To view these traces, click on the Jaeger link icon in the topology. The icon is a little box with an outgoing arrow (Figure 8). You might have to log in again the first time you check the traces.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig8.png?itok=Wi3se1IX" width="600" height="288" alt="An arrow points to the Jaeger link icon in the topology view." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: Traces are available for a component when a small box icon appears at the top right of the component. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;The icon takes you to the Jaeger UI (Figure 9), where you can filter traces based on the service called &lt;code&gt;fruits&lt;/code&gt; (set in our  &lt;code&gt;tracer.js &lt;/code&gt;configuration) and identify the error:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enter &lt;code&gt;fruits&lt;/code&gt; in the &lt;strong&gt;Service&lt;/strong&gt; box.&lt;/li&gt; &lt;li&gt;Enter &lt;code&gt;POST /api/fruits &lt;/code&gt;in the &lt;strong&gt;Operation&lt;/strong&gt; box.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Find traces&lt;/strong&gt; button.&lt;/li&gt; &lt;/ul&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig9.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig9.png?itok=Fo8i0Lit" width="600" height="334" alt="Illustration of the Jaeger form." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Fill out Jaeger's form as described in the text. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Click on the error trace to view all the operations passing through Express and its middleware up to the database (Figure 10).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig10.png?itok=W7ZiSzlk" width="600" height="294" alt="The Jaeger UI shows a history of operations after clicking the error trace." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Jaeger shows everything that happened up until the call reaches the database. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Click on the error to view more specific details (Figure 11).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Trace-nodejs-fig11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Trace-nodejs-fig11.png?itok=OO_mqs80" width="600" height="242" alt="A screenshot of a list of details about an error." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: Error details include a cause statement and the error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Jaeger provides the SQL statement. Here you can double-check the code and the error message on the &lt;code&gt;otel.status_description&lt;/code&gt; line: "relation 'product0' does not exist."&lt;/p&gt; &lt;p&gt;This information reveals that, although the error was reported from the database component, the problem springs from the application, which specified a table that does not exist. This information allows you to go back to the application and fix the bug.&lt;/p&gt; &lt;p&gt;Although this example is a bit contrived, it illustrates the level of information provided by auto-instrumentation, as well as the power of connecting the information provided with the flow of the request through the application's components.&lt;/p&gt; &lt;p&gt;Another benefit of OpenTelemetry is that the same trace for the &lt;code&gt;/api/fruits&lt;/code&gt; request shows the time spent in the &lt;code&gt;pg:query:select&lt;/code&gt; step. If this step creates a performance problem, you might be able to resolve it by adding an additional index to the products table.&lt;/p&gt; &lt;h2&gt;OpenTelemetry benefits networked applications&lt;/h2&gt; &lt;p&gt;This article illustrated how OpenTelemetry tracing increases observability for a Node.js deployment in OpenShift. The &lt;code&gt;tracer.js&lt;/code&gt; example demonstrated:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;That operators provided by Red Hat were easily installed in OpenShift, creating individual OpenTelemetry Collector and Jaeger instances for an application.&lt;/li&gt; &lt;li&gt;The addition of auto-instrumentation plugins for common &lt;a href="https://github.com/open-telemetry/opentelemetry-js-contrib/tree/main/plugins/node"&gt;Node.js packages&lt;/a&gt; to an existing Node.js application.&lt;/li&gt; &lt;li&gt;The captured traces answered two key questions: Where is the problem and what is causing the error? In our example, the answers were: The problem was located in the database layer source code and a typo in an SQL statement caused that bug.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has this piqued your interest in trying OpenTelemetry in your environment? We hope this article has helped you understand how you to use OpenTelemetry with a Node.js application deployed to OpenShift.&lt;/p&gt; &lt;p&gt;Read more about &lt;a href="https://www.ibm.com/cloud/learn/observability"&gt;observability&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.10/distr_tracing/distributed-tracing-release-notes.html#distr-tracing-product-overview_distributed-tracing-release-notes"&gt;Red Hat distributed tracing&lt;/a&gt;, and &lt;a href="https://opentelemetry.io/docs/instrumentation/js/"&gt;OpenTelemetry&lt;/a&gt;. To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/23/how-use-opentelemetry-trace-nodejs-applications" title="How to use OpenTelemetry to trace Node.js applications"&gt;How to use OpenTelemetry to trace Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Annapurna Patil, Helio Frota, Rashmi Panchamukhi</dc:creator><dc:date>2022-08-23T07:00:00Z</dc:date></entry><entry><title>How to program a multitenant SaaS platform in Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/22/how-program-multitenant-saas-platform-kubernetes" /><author><name>Bob Reselman</name></author><id>5982b94e-d5cc-4ec0-aea6-71f90efa4199</id><updated>2022-08-22T07:00:00Z</updated><published>2022-08-22T07:00:00Z</published><summary type="html">&lt;p&gt;In a &lt;a href="https://developers.redhat.com/articles/2022/08/12/implement-multitenant-saas-kubernetes"&gt;previous article&lt;/a&gt;, I described how to create a SaaS platform in a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster. This article takes a detailed look inside the demonstration project that accompanied that earlier article. The demonstration project is the code base shared by all tenants using the SaaS platform. This article describes the structure of the demonstration application. You'll also see how to get the code up and running for multiple &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; tenants in a Kubernetes cluster, and how to expose the service to clients.&lt;/p&gt; &lt;p&gt;A key aspect of SaaS architecture is a generic code base used by all tenants running in the Kubernetes cluster. The application logic used by each tenant is encapsulated in a Linux container image that's declared within the definition of the tenant's Kubernetes deployment. We'll see multiple examples of this generic approach and its benefits in this article.&lt;/p&gt; &lt;p&gt;The Linux container for each tenant is configured by setting a standard set of environment variables to values specific to that tenant. As shown in the previous article, adding a new tenant to the SaaS platform involves nothing more than setting up a database and creating a Kubernetes Secret, deployment, service, and route resources, all of which are assigned to a Kubernetes namespace created especially for the tenant. The result is that a single code base can support any number of tenants.&lt;/p&gt; &lt;h2&gt;Purpose of the demonstration project&lt;/h2&gt; &lt;p&gt;The demonstration project (which you can &lt;a href="https://github.com/redhat-developer-demos/instrument-resellers/"&gt;download from GitHub&lt;/a&gt;) is an evolution of code that started out as a single, standalone application used by a company named Clyde's Clarinets. The company determined that its code logic was generic enough to be converted to a SaaS platform that could support a number of other instrument resellers. Thus, Clyde's Clarinets became the Instrument Resellers SaaS platform.&lt;/p&gt; &lt;p&gt;The Resellers SaaS platform enables each vendor to acquire, refurbish, and resell a type of musical instrument special to that vendor. The Instrument Resellers demonstration project supports three vendors: Clyde's Clarinets, Betty's Brass, and Sidney's Saxophones (see Figure 1.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_6.png?itok=PrS6HW8G" width="385" height="565" alt="Web sites for multiple tenants using different namespaces can be supported by a single platform." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Web sites for multiple tenants using different namespaces can be supported by a single platform. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The sections that follow describe the design and code of the Instrument Reseller SaaS platform. Be advised that the demonstration code is still a work in progress. Most of the initial design work has been done and the endpoints for the &lt;code&gt;GET&lt;/code&gt; methods to the API resources are fully functional. Also, the project ships with a data seeding feature that makes it possible for each tenant's API instance to return instrument data that is specific to the reseller. However, the workflow code that moves an instrument through its phases, from Acquisition to Refurbishment and finally to Purchase, still needs to be written. There also has to be logic for &lt;code&gt;POST&lt;/code&gt;, &lt;code&gt;PUT&lt;/code&gt;, and &lt;code&gt;DELETE&lt;/code&gt; actions in the API.&lt;/p&gt; &lt;p&gt;Still, for the purposes of this article, the demonstration code provides a good starting place to understand how to use Kubernetes namespaces to implement a multitenant application in a single Kubernetes cluster. Once you get the code up and running in a Kubernetes cluster, you can experiment with adding the features that still need to be implemented.&lt;/p&gt; &lt;h2&gt;Designing the Instrument Resellers SaaS Platform&lt;/h2&gt; &lt;p&gt;The Instrument Resellers SaaS platform is written using the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; framework. The code exposes a RESTful API that represents resources for acquiring, refurbishing, and reselling musical instruments. Each tenant using the SaaS platform exposes an instance of the API. For example, Sidney's Saxophones has its own instance of the API, as does Clyde's Clarinets and Betty's Brass. Figure 2 shows a user interface (UI) that exposes the &lt;code&gt;GET&lt;/code&gt; operations in Sidney's Saxophones.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ui_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ui_2.png?itok=x14sAN4I" width="1153" height="622" alt="Each instance of a vendor on the Instrument Resellers SaaS platform is represented by a dedicated RESTful API." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Each instance of a vendor on the Instrument Resellers SaaS platform is represented by a dedicated RESTful API. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The structure of the API is generic and is described in a &lt;a href="https://github.com/reselbob/instrument-resellers/blob/main/src/api/openapi.yaml"&gt;YAML file&lt;/a&gt; formatted according to the &lt;a href="https://swagger.io/specification/"&gt;OpenAPI 3.0 specification&lt;/a&gt;. As mentioned previously, the purpose of the Instrument Resellers SaaS is to allow a vendor to acquire, refurbish, and resell an instrument. The API represents these generic resources as Acquisitions, Refurbishments, and Purchases. (The Purchases resource describes instruments that have been sold via resale.)&lt;/p&gt; &lt;p&gt;The underlying logic assumes that once an instrument is acquired it will need to be refurbished, even if it only requires a cleaning. A Refurbishment is specified with &lt;code&gt;startDate&lt;/code&gt; and &lt;code&gt;finishDate&lt;/code&gt; properties. Once a Refurbishment is assigned a &lt;code&gt;finishDate&lt;/code&gt;, the associated instrument is ready for sale. Thus, the availability of an instrument for sale is determined through implication: if it has a &lt;code&gt;finishDate&lt;/code&gt; value, it can be sold. Once an instrument is sold, it becomes a Purchase resource.&lt;/p&gt; &lt;p&gt;There is a good argument to be made that instead of relying upon inference to determine that a Refurbishment is ready for sale, the developer experience would be improved through a more explicit approach, such as moving the instrument into a RESTful resource named Inventory. Making such a change would not degrade the generic status of the API, and creating an Inventory resource would not corrupt the intention of the SaaS, because all resellers—whether they're reselling clarinets, drums, or guitars—could support an Inventory resource. However, the internal logic in the source code for the SaaS would need to be altered to create an Inventory resource once a Refurbishment is assigned a &lt;code&gt;finishDate&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Again, making such changes would be OK because the change is generic. The important thing to remember is that keeping the API generic is essential to the design of the SaaS. Were the API to become too explicit, it would become brittle and lose its value for a SaaS platform.&lt;/p&gt; &lt;h2&gt;Defining a versatile data schema&lt;/h2&gt; &lt;p&gt;The need for a generic approach also holds true when designing the various data schemas used in the SaaS. Figure 3 shows the schemas used by the Instrument Resellers SaaS Platform.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/schema.png?itok=gmVxkdaH" width="979" height="589" alt="The platform defines a schema for each resource (Instrument, etc.)." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The platform defines a schema for each resource (Instrument, etc.). &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The platform defines a schema for each resource.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The important thing to note about the data schemas is that the data types are generic. Along with scalar types such as &lt;code&gt;string&lt;/code&gt; and &lt;code&gt;decimal&lt;/code&gt;, there are complex types for &lt;code&gt;Manufacturer&lt;/code&gt;, &lt;code&gt;Address&lt;/code&gt;, &lt;code&gt;User&lt;/code&gt;, &lt;code&gt;Acquisition&lt;/code&gt;, &lt;code&gt;Refurbishment&lt;/code&gt;, and &lt;code&gt;Purchase&lt;/code&gt;. The complex types are designed to apply to all instrument types and locales. Thus, the &lt;code&gt;Address&lt;/code&gt; type can be used for a location in the U.S. as well as a location in France. Also, the &lt;code&gt;Instrument&lt;/code&gt; type uses the data type &lt;code&gt;string&lt;/code&gt; to describe the &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;instrument&lt;/code&gt;, and &lt;code&gt;type&lt;/code&gt; properties.&lt;/p&gt; &lt;p&gt;To see how a particular tenant uses the schema, we'll retrieve the information for one clarinet offered by Clyde's Clarinets:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets.local/v1/instruments/62bb6dde9b6a0fb486702123&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output from this command shows that the clarinet has a type of standard B flat and the name Excellent Clarinet. The instrument was manufactured by Hanson Clarinet Company. The address of Hanson Clarinet Company is displayed in JSON assigned to the address property.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "_id": "62bb6dde9b6a0fb486702123", "name": "Excellent Clarinet", "instrument": "clarinet", "type": "standard B flat", "manufacturer": { "name": "Hanson Clarinet Company", "description": "Noted for their clarinets which are made in their workshops in Marsden, West Yorkshire", "address": { "address_1": "Warehouse Hill", "address_2": "Apt. 745", "city": "Marsden", "state_province": "West Yorkshire", "zip_region_code": "HD7 6AB", "country": "UK", "_id": "62bb6dde9b6a0fb486702125", "created": "2022-06-28T21:08:46.549Z" }, "_id": "62bb6dde9b6a0fb486702124" }, "__v": 0 }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using a string for the &lt;code&gt;type&lt;/code&gt; property allows any vendor to describe an instrument with a good deal of distinction, regardless of whether the instrument is a guitar, a drum, or a saxophone.&lt;/p&gt; &lt;p&gt;Again, the trick with defining data structures is to keep the schemas generic. Just like the API, if a schema becomes too specific, it becomes brittle and subject to breakage.&lt;/p&gt; &lt;h2&gt;Deploying the demonstration application&lt;/h2&gt; &lt;p&gt;Now that you've learned about the structure of the RESTful API published by the Instrument Reseller SaaS platform as well as the data schemas used by the API, you're ready to get the demonstration code up and running.&lt;/p&gt; &lt;h3&gt;Identifying the runtime environment&lt;/h3&gt; &lt;p&gt;The demonstration code is intended to run on an instance of the &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; or &lt;a href="https://docs.fedoraproject.org/en-US/fedora/latest/install-guide/"&gt;Fedora&lt;/a&gt; operating system with &lt;a href="https://microshift.io/"&gt;MicroShift&lt;/a&gt; installed. MicroShift is a scaled-back version of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. The reason we use MicroShift is that it offers a convenient way to deploy Kubernetes while providing the OpenShift &lt;a href="https://docs.openshift.com/container-platform/3.11/rest_api/route_openshift_io/route-route-openshift-io-v1.html"&gt;route&lt;/a&gt; resource, which provides an easy way to bind a Kubernetes service to a public URL that is accessible from outside the Kubernetes cluster. This &lt;a href="https://microshift.io/docs/getting-started/"&gt;Getting Started page&lt;/a&gt; explains how to install MicroShift on a Red Hat operating system.&lt;/p&gt; &lt;p&gt;Also, the demonstration code is designed to use a MongoDB database. A later section shows some of the details of working with MongoDB.&lt;/p&gt; &lt;p&gt;The deployment process is facilitated by using Kubernetes manifest files. You'll examine the details of the manifest files in a moment. But first, let's cover the demonstration project's data seeding feature.&lt;/p&gt; &lt;h3&gt;Data seeding&lt;/h3&gt; &lt;p&gt;The demonstration application seeds itself with random data that is particular to the instrument type that each reseller supports. For example, when the reseller Clyde's Clarinets is deployed into the Kubernetes cluster, the deployment seeds that reseller with data about acquiring, refurbishing, and reselling clarinets. The Sidney's Saxophones deployment seeds data relevant to saxophones. Betty's Brass is seeded with data relevant to brass instruments.&lt;/p&gt; &lt;p&gt;The purpose of data seeding is to provide a concrete way to understand multitenancy during this demo application. When you exercise the API for Clyde's Clarinets, you'll see only data relevant to Clyde's Clarinets. The same is true for Betty's Brass and Sidney's Saxophones. Seeing relevant data in a concrete manner makes it easier to understand the concept behind supporting multiple tenants in a SaaS platform.&lt;/p&gt; &lt;p&gt;The actual seeding process is facilitated by a Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"&gt;init container&lt;/a&gt; that executes as part of the Kubernetes deployment. An init container is a container that runs before any other containers that are part of the Kubernetes pod. Our particular init container seeds the MongoDB database defined for the given reseller by loading the database with random data appropriate for the tenant (see Figure 4.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/seed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/seed.png?itok=vW00WJr5" width="415" height="223" alt="The demonstration code for the Instrument Resellers SaaS platform seeds data particular to a given reseller." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The demonstration code for the Instrument Resellers SaaS platform seeds data particular to a given reseller. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The data seeding pattern has one small risk because the init container runs for every replica in the cluster. Unless some precaution is taken, the init container will write more data to the database each time a new replica starts, even though we need to run the init container only once. So the seeder used in the demonstration project checks for the existence of seed data and refrains from adding redundant entries.&lt;/p&gt; &lt;p&gt;The application binds to the MongoDB server via a connection string URL. That URL can represent a MongoDB server running internally within the Kubernetes cluster or external to the cluster using a service such as &lt;a href="https://www.mongodb.com/atlas/database"&gt;MongoDB Atlas&lt;/a&gt;. For simplicity's sake, the demonstration code was tested using a MongoDB instance running on MongoDB Atlas. Each tenant in the SaaS platform, in this example, is bound to a MongoDB instance as part of the process of configuring the Kubernetes manifest file for the given tenant's deployment.&lt;/p&gt; &lt;h3&gt;Getting Kubernetes manifest files&lt;/h3&gt; &lt;p&gt;The logic that powers each tenant in the Instrument Resellers SaaS platform is encapsulated in container images that are stored on the &lt;a href="https://quay.io"&gt;Quay&lt;/a&gt; container registry. You don't need to fiddle with source code directly to get an instrument reseller up and running. But you do need to configure the containers.&lt;/p&gt; &lt;p&gt;Configuration operates through Kubernetes manifest files that specify properties in YAML. The configuration files for this example are stored in a GitHub source code repository.&lt;/p&gt; &lt;p&gt;To get the manifest files, go to a terminal window on the machine in which MicroShift is running and execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ git clone https://github.com/redhat-developer-demos/instrument-resellers&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command copies the source code down from the GitHub repository that hosts the demonstration project.&lt;/p&gt; &lt;p&gt;Once the code is cloned from GitHub, navigate into the source code directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ cd instrument-resellers&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You're now ready to configure the Kubernetes manifest files.&lt;/p&gt; &lt;h3&gt;Preparing the manifest files for deployment&lt;/h3&gt; &lt;p&gt;The manifest files that you'll use to create instrument resellers in the Kubernetes cluster are in the &lt;code&gt;instrument-resellers/openshift&lt;/code&gt; directory. There, you'll find the manifest file that creates a tenant for each instrument reseller. The declarations for the namespace, Secret, deployment, service, and route resources for the given reseller are combined into a single YAML file for that reseller. The manifest file for Clyde's Clarinets is named &lt;code&gt;clarinet.yaml&lt;/code&gt;, the manifest file for Betty's Brass is &lt;code&gt;brass.yaml&lt;/code&gt;, and the file for Sidney's Saxophones is &lt;code&gt;saxophone.yaml&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The essential task performed by each resource follows:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;namespace&lt;/code&gt;: Declares the namespace that is unique for the given reseller.&lt;/li&gt; &lt;li&gt;&lt;code&gt;deployment&lt;/code&gt;: Configures the init container that seeds the reseller data and the regular container that has the application logic.&lt;/li&gt; &lt;li&gt;&lt;code&gt;service&lt;/code&gt;: Exposes the reseller on the internal Kubernetes network.&lt;/li&gt; &lt;li&gt;&lt;code&gt;route&lt;/code&gt;: Provides the URL to access the reseller from outside the Kubernetes cluster.&lt;/li&gt; &lt;li&gt;&lt;code&gt;secret&lt;/code&gt;: Specifies the URL (with embedded username and password) that defines the connection to the external MongoDB instance in which the reseller's data is stored.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The following excerpt from &lt;code&gt;clarinet.yaml&lt;/code&gt; (which is &lt;a href="https://github.com/redhat-developer-demos/instrument-resellers/blob/main/openshift/clarinet.yaml"&gt;in the GitHub repository for this demo application&lt;/a&gt;) shows the declaration for the Kubernetes Secret that has the URL that will connect the application code for Clyde's Clarinets to its associated MongoDB instance. Note that the &lt;code&gt;stringData.url&lt;/code&gt; property is assigned the value &lt;code&gt;&lt;mongo-url-here&gt;&lt;/code&gt;. This value is a placeholder for the URL that will be provided by the developer.&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: clydesclarinets type: Opaque stringData: url: &lt;mongo-url-here&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The demonstration project ships with a utility script named &lt;code&gt;set_mongo_url&lt;/code&gt;. The script is provided as a convenience. Executing the script inserts the connection string URL in the manifest files for all the instrument resellers: Clyde's Clarinets, Betty's Brass, and Sidney's Saxophones.&lt;/p&gt; &lt;p&gt;Or course, the script assumes that all the resellers use the same instance of MongoDB. In a production situation, each instrument reseller might be bound to a different database. Thus, the connection URLs will differ among resellers. But in this example, for demonstration purposes, using a single MongoDB instance is fine. Both the seeder code and the API code for a given tenant know how to create their particular database within the MongoDB instance. The database name for each reseller is defined by configuring an environment variable named &lt;code&gt;RESELLER_DB_NAME&lt;/code&gt; that is common to all resellers.&lt;/p&gt; &lt;p&gt;The syntax for using the &lt;code&gt;set_mongo_url&lt;/code&gt; utility script follows. Substitute your own connection string URL for &lt;code&gt;&lt;connection_string&gt;&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ sh set_mongo_url &lt;connection_string&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thus, if you want to make all the resellers in the demonstration project use the MongoDB instance defined bythe URL &lt;code&gt;mongodb+srv://reseller_user:F1Tc4lO5IVAXYZz@cluster0.ooooeo.mongodb.net&lt;/code&gt;, execute the utility script like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ sh set_mongo_url mongodb+srv://reseller_user:F1Tc4lO5IVAXYZz@cluster0.ooooeo.mongodb.net&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Applying the manifest files to the Kubernetes cluster&lt;/h2&gt; &lt;p&gt;Once the MongoDB URL has been specified for all the manifest files using the utility script in the previous section, the next step is to apply the manifest file for each reseller to the Kubernetes cluster. Create an instance of each instrument reseller in the MicroShift Kubernetes cluster by executing a &lt;code&gt;kubectl apply&lt;/code&gt; command for that instrument reseller.&lt;/p&gt; &lt;p&gt;Run the following command to create the Clyde's Clarinets reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f clarinet.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create the Betty's Brass reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f brass.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command to create the Sidney's Saxophones reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ kubectl apply -f saxophone.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting the application routes&lt;/h2&gt; &lt;p&gt;After the three resellers have been deployed using &lt;code&gt;kubectl apply&lt;/code&gt;, you need to get the URL through which clients can get access to each of them. OpenShift makes this task simple through the &lt;code&gt;oc get routes&lt;/code&gt; command. Go to the Red Hat Enterprise Linux or Fedora instance that hosts the MicroShift Kubernetes cluster, and execute that command for each reseller to get its route. The &lt;code&gt;HOST/PORT&lt;/code&gt; column in the output lists the route's URL.&lt;/p&gt; &lt;p&gt;To get the route to Clyde's Clarinets, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n clydesclarinets&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller clydesclarinets.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the route to Betty's Brass, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n bettysbrass&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller bettysbrass.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the route to Sidney's Saxophones, execute:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ oc get route -n sidneyssaxophones&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get output similar to:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD instrumentreseller sidneyssaxophones.local instrumentreseller 8088 None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that all the routes are retrieved according to the relevant Kubernetes namespace. To get the route for Clyde's Clarinets, you had to use &lt;code&gt;-n clydesclarinets&lt;/code&gt;. To get the route for Betty's Brass, you had to specify the &lt;code&gt;bettysbrass&lt;/code&gt; namespace. And to get the route for Sidney's Saxophones you had to specify the &lt;code&gt;sidneyssaxophones&lt;/code&gt; namespace.&lt;/p&gt; &lt;p&gt;This all makes sense when you remember that tenant isolation in the Kubernetes cluster is achieved through namespaces. Access to the route for each of the instrument resellers is determined according to its particular namespace.&lt;/p&gt; &lt;h2&gt;Binding the route's domain name to the machine host&lt;/h2&gt; &lt;p&gt;The last thing that needs to be done to access a particular instrument reseller API within the Kubernetes cluster is to bind the domain name of each instrument reseller to the IP address of the machine on which the Kubernetes cluster is running. The domain name returned by the &lt;code&gt;oc get route -n &lt;namespace&gt;&lt;/code&gt; command is automatically mapped to the associated service within the Kubernetes cluster. However, outside of the cluster, an instrument reseller's domain name is nothing more than an arbitrary string. By default, the host computer has no understanding of how to route the domain name to an IP address—the host computer's IP address, in this case.&lt;/p&gt; &lt;p&gt;Domain naming is not magical. Whenever a domain name is in play, that name is bound to an IP address of a web server or load balancer somewhere. The scope of the domain name can vary. If the host with that domain name is on the Web, the domain name is bound to a particular IP address by a domain registrar and propagated to all the public domain name servers across the globe.&lt;/p&gt; &lt;p&gt;If the scope of the domain name is limited to a local network, that domain name is bound to an IP address on the local network by making an entry in that local network's domain name server. If the scope of the domain name is limited to a single computer, that domain name is bound to that computer's IP address through an entry in the &lt;code&gt;/etc/hosts&lt;/code&gt; file of the computer using the domain name. Because MicroShift includes a Domain Name System (DNS) server, it can find the IP address in the host's &lt;code&gt;/etc/hosts&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;With regard to the demonstration project, the scope of the domain names in the URLs retrieved from the Kubernetes cluster using &lt;code&gt;oc get route&lt;/code&gt; is local to the computer running the Kubernetes cluster. Thus, at the least, that domain name needs to be bound to the local machine by making an entry in the &lt;code&gt;/etc/hosts&lt;/code&gt; file. The following lines in &lt;code&gt;/etc/hosts&lt;/code&gt; bind the domain names of our three instrument resellers to a local host with an IP address of 192.168.86.32:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;192.168.86.32 clydesclarinets.local 192.168.86.32 bettysbrass.local 192.168.86.32 sidneyssaxophones.local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once &lt;code&gt;/etc/hosts&lt;/code&gt; is updated, you can access the given instrument reseller using the cURL command on the computer hosting the Kubernetes cluster. For example, using the domain names retrieved by using &lt;code&gt;oc get routes&lt;/code&gt; earlier, you can query the &lt;code&gt;/healthcheck&lt;/code&gt; endpoint on each instrument reseller's API to determine whether the given instrument reseller service is up and running.&lt;/p&gt; &lt;p&gt;For instance, the following command performs a health check on Sidney's Saxophones:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl sidneyssaxophones.local/v1/healthcheck&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The call to the API endpoint should produce the following results:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "date": "2022-07-05T20:20:28.519Z", "message": "Things are A-OK at Sidney's Saxophones" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command performs a health check on Clyde's Clarinets:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets.local/v1/healthcheck&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is particular to the clarinet reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;{ "date": "2022-07-05T20:24:57.710Z", "message": "Things are A-OK at Clyde's Clarinets" }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An interesting point to note is that a port number isn't required along with the instrument reseller's domain name to query a reseller's API. The reason for this is that the server running OpenShift has intelligence that examines the domain name associated with the HTTP request and routes the request to the relevant tenant according to the domain name. This technique is called &lt;a href="https://en.wikipedia.org/wiki/Virtual_hosting#Name-based"&gt;name-based virtual hosting&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When name-based virtual hosting is in force, multiple domain names can be served from the same network port. The server reads the host property in the incoming request's HTTP header and then maps the domain name defined in the header to an internal IP address and port number within the Kubernetes cluster, according to the particular domain name.&lt;/p&gt; &lt;p&gt;There is another interesting point about the calls to the health check. When you look at the source code that is common to all instrument reseller tenants running in the Kubernetes cluster, you'll see that the differences in responses between Sidney's Saxophones and Clyde's Clarinets are due to differences in configuration. The code running both instrument resellers is identical.&lt;/p&gt; &lt;h2&gt;Exercising the tenant APIs&lt;/h2&gt; &lt;p&gt;As mentioned many times in this article, a significant benefit of a multitenant SaaS platform is that one code base can support a variety of tenants. The queries performing health checks in the previous section are a good example of this benefit. The benefit becomes even more apparent when making calls to the resource endpoints of the API for a given instrument reseller.&lt;/p&gt; &lt;p&gt;For example, the following command asks Sidney's Saxophones API for Purchases:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl sidneyssaxophones.local/v1/purchases&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;[ { "_id": "62bb6d9215e72a14688a16ba", "purchaseDate": "2022-05-02T12:53:46.088Z", "created": "2022-06-28T21:07:30.044Z", "buyer": { "firstName": "Meghan", "lastName": "Boyer", "email": "Meghan_Boyer92@hotmail.com", "phone": "758-676-6625 x849", "userType": "BUYER", "address": { "address_1": "69709 Renner Plains", "address_2": "Suite 351", "city": "Vallejo", "state_province": "AZ", "zip_region_code": "38547", "country": "USA", "_id": "62bb6d9215e72a14688a16bc", "created": "2022-06-28T21:07:30.044Z" }, "_id": "62bb6d9215e72a14688a16bb", "created": "2022-06-28T21:07:30.044Z" }, "instrument": { "instrument": "saxophone", "type": "bass", "name": "Twin Saxophone", "manufacturer": { "name": "Cannonball Musical Instruments", "description": "Manufacturer of a wide range of musical instruments", "address": { "address_1": "625 E Sego Lily Dr.", "address_2": "Apt. 276", "city": "Sandy", "state_province": "UT", "zip_region_code": "84070", "country": "USA", "_id": "62bb6d9215e72a14688a16bf", "created": "2022-06-28T21:07:30.044Z" }, "_id": "62bb6d9215e72a14688a16be" }, "_id": "62bb6d9215e72a14688a16bd" }, "price": 827, "__v": 0 }, . . . ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Likewise, the following command queries the Clyde's Clarinets API:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ curl clydesclarinets-clydesclarinets.cluster.local/v1/purchases&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output shows the Purchases for that reseller:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;[ { "_id": "62bb6dd79b6a0fb4867020a8", "purchaseDate": "2022-05-02T19:59:04.324Z", "created": "2022-06-28T21:08:39.301Z", "buyer": { "firstName": "Otha", "lastName": "Bashirian", "email": "Otha.Bashirian57@gmail.com", "phone": "(863) 541-6638 x8875", "userType": "BUYER", "address": { "address_1": "47888 Oren Wall", "address_2": "Apt. 463", "city": "Dublin", "state_province": "FL", "zip_region_code": "49394", "country": "USA", "_id": "62bb6dd79b6a0fb4867020aa", "created": "2022-06-28T21:08:39.302Z" }, "_id": "62bb6dd79b6a0fb4867020a9", "created": "2022-06-28T21:08:39.302Z" }, "instrument": { "instrument": "clarinet", "type": "soprano", "name": "Ecstatic Clarinet", "manufacturer": { "name": "Amati-Denak", "description": "A manufacturer of wind and percussion instruments, parts, and accessories.", "address": { "address_1": "Dukelská 44", "address_2": "Apt. 117", "city": "Kraslice", "state_province": "Sokolov", "zip_region_code": "358 01", "country": "CZ", "_id": "62bb6dd79b6a0fb4867020ad", "created": "2022-06-28T21:08:39.302Z" }, "_id": "62bb6dd79b6a0fb4867020ac" }, "_id": "62bb6dd79b6a0fb4867020ab" }, "price": 777, "__v": 0 }, . . . ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The data differs by the type of instrument sold by the reseller. But when you look under the covers at the application logic, you'll see that the code used by Sidney's Saxophones and Clyde's Clarinets is identical. The queries illustrate yet another example of the beauty of a well-designed multitenant SaaS platform. One single code base supports as many tenants as the physical infrastructure can host.&lt;/p&gt; &lt;h2&gt;Lessons regarding SaaS&lt;/h2&gt; &lt;p&gt;The demonstration project described in the article shows that multitenant SaaS platforms offer significant benefits. First and foremost is that when designed properly, a single code base can support any number of tenants running in a SaaS platform. Instead of having to dedicate a number of developers to many different software projects, a SaaS platform requires only a single development team supporting a single code base. The cost savings and reduction in technical debt are noteworthy.&lt;/p&gt; &lt;p&gt;Adding a new tenant to a SaaS platform running under Kubernetes requires nothing more than identifying a data source and configuring a set of Kubernetes resources for the new tenant. Deployment can be a matter of minutes instead of hours or even days. By saving time, you will save money.&lt;/p&gt; &lt;p&gt;Yet, for all the benefits that a SaaS platform provides, it also creates challenges.&lt;/p&gt; &lt;p&gt;The first challenge is getting configuration settings right. One misconfigured URL to a database or one bad value assignment to an environment variable can incur hours of debugging time. Configuration settings always need to be correct. Hence, automation is a recommended best practice.&lt;/p&gt; &lt;p&gt;The second challenge concerns infrastructure considerations. Optimal performance requires that the physical infrastructure on which the code runs can support the tenant's anticipated load. This means making sure that the physical infrastructure has the CPU, storage, and network capacity to support all tenants and that the Linux containers running the application logic are configured to take advantage of the physical resources available. Achieving this diversity can be complicated when each tenant is using the same code base.&lt;/p&gt; &lt;p&gt;A &lt;a href="https://www.redhat.com/en/topics/microservices/what-is-a-service-mesh"&gt;service mesh&lt;/a&gt; can make the tenant more operationally resilient by implementing circuit breaking and rerouting in the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;In conclusion, the key takeaways for making a multitenant SaaS platform work under Kubernetes and OpenShift are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Design code and data structures that are generic enough to support a variety of tenants.&lt;/li&gt; &lt;li&gt;Make sure that the environment hosting the SaaS platform is robust and resilient enough to support the loads of all clients.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/08/22/how-program-multitenant-saas-platform-kubernetes" title="How to program a multitenant SaaS platform in Kubernetes"&gt;How to program a multitenant SaaS platform in Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-08-22T07:00:00Z</dc:date></entry></feed>
