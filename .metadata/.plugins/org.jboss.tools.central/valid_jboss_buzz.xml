<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Improve UDP performance in RHEL 8.5</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/STb7ANGMifM/improve-udp-performance-rhel-85" /><author><name>Paolo Abeni</name></author><id>9a9e3a72-5898-4989-b1ab-94d81ece8ae1</id><updated>2021-11-05T07:00:00Z</updated><published>2021-11-05T07:00:00Z</published><summary type="html">&lt;p&gt;If you have ever tested throughput performance in a container deployment, you know that the UDP protocol is (a lot) slower than TCP. How can that be possible? After all, the TCP protocol is extremely complex, whereas UDP is simple and carries less per-packet overhead. This article will explain the not-so-dark magic beyond the superior TCP throughput performance and how recent improvements in the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel can close that gap. You'll also learn how to use all these shiny new features in the upcoming version 8.5 of &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; to boost UDP throughput in container deployments by a factor of two or more.&lt;/p&gt; &lt;h2&gt;Bulk transfers and packet aggregation through TSO and GRO&lt;/h2&gt; &lt;p&gt;The typical container network infrastructure is quite complex. When a container sends a packet, it traverses the kernel network stack inside the container itself, reaches a virtual Ethernet (veth) device, and is forwarded on the peer veth device to reach the host virtual network. The host then forwards the packet towards a Linux or &lt;a href="https://www.openvswitch.org/"&gt;Open vSwitch (OVS)&lt;/a&gt; bridge and eventually over a UDP tunnel. Finally, the packet reaches the hardware network interface card (NIC). Figure 1 illustrates the sequence of packet transmissions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/sequence.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/sequence.png?itok=JgulxvNW" width="675" height="360" alt="A network packet in a container must pass through many software components to reach the hardware device that sends it out on the network." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A network packet in a container must pass through many software components to reach the hardware device that sends it out on the network. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;A packet coming in on the wire, targeting the container, will go through the same path in the reverse direction.&lt;/p&gt; &lt;p&gt;It's easy to guess that the CPU time needed to perform all the forwarding and encapsulation steps is easily far greater than the actual transport protocol processing time, regardless of the actual complexity of the protocol—whether UDP, TCP, or MPTCP. Additionally, the container orchestration adds significant overhead due to the complex forwarding ruleset needed to cope with multiple containers per host, etc.&lt;/p&gt; &lt;p&gt;TCP and MPTCP can alleviate the forwarding overhead thanks to aggregation. On the transmit side, the data stream is glued into packets larger than the current TCP maximum segment size (MSS). The large packets traverse the whole virtual network unmodified until they land on the real NIC. In the most common scenario, the NIC itself segments the large packet into MSS-size ones via Transmit Segmentation Offload (TSO).&lt;/p&gt; &lt;p&gt;In the reverse path, the packets received by the NIC are aggregated into large ones before entering the network stack. The received packet aggregation is usually performed by the CPU and is called Generic Receive Offload (GRO). Some NICs have hardware offload capability for GRO too.&lt;/p&gt; &lt;p&gt;In both cases (receive and transmit), a single aggregated packet amortizes the high virtual network forwarding cost for multiple packets received or sent on the wire. The key point is that such aggregation is not available by default for the UDP protocol, and each UDP packet has to pay the full forwarding overhead.&lt;/p&gt; &lt;h2&gt;Generic Receive Offload for UDP—why not?&lt;/h2&gt; &lt;p&gt;The TCP protocol is stream-oriented. Therefore, its data can be segmented in as many packets as needed, as long as the data stream can be reconstructed while preserving the data integrity. In contrast, the UDP protocol is packet-based: The data is transmitted in units (packets) whose size is specified by the user-space application. Splitting a single UDP packet into multiple ones or aggregating multiple UDP packets in a single one may confuse the recipient, which relies on the packet length to identify the application layer message size.&lt;/p&gt; &lt;p&gt;Due to all the considerations discussed so far, the Linux kernel has long supported GRO/TSO for the TCP protocol only.&lt;/p&gt; &lt;p&gt;Since Linux 4.18, thanks to QUIC—a reliable transport protocol built on top of UDP—the Linux UDP implementation has gained TSO support. It's up to the application to enable UDP segmentation on a per-socket basis and then pass the aggregated UDP packets to the kernel and the target length for the on-the-wire packet. Because this feature is a per-application opt-in, the peers by design understand that the application message size is potentially different from the transmitted or received UDP packet size.&lt;/p&gt; &lt;p&gt;TSO support can significantly improve the performance of UDP transmits in containers. But the improvement doesn't assist the receive path.&lt;/p&gt; &lt;p&gt;More recently, in Linux 5.10, the UDP protocol has additionally gained GRO support. This side of the aggregation process is also enabled on a per-socket basis. Once the application sets the relevant socket option, &lt;code&gt;UDP_GRO&lt;/code&gt;, the network stack starts aggregating incoming packets directed to the relevant socket in a process similar to what it was already doing for TCP.&lt;/p&gt; &lt;p&gt;With GRO in place, even the container receive path could potentially see the benefit of packets aggregation. Still, a few significant problems stand out. Because both TSO and GRO must be enabled explicitly from the user-space applications, only a few would go to the trouble. Additionally, a virtual network is often built on top of UDP tunnel virtual devices, and the initial UDP GRO implementation did not support them.&lt;/p&gt; &lt;h2&gt;The missing Linux kernel pieces&lt;/h2&gt; &lt;p&gt;But the Linux network community never sleeps—literally, have a look at the timestamps on the email sent by the main contributors on the mailing list—and system-wide UDP GRO support has long been only a few bits away: the kernel just needed to segment back the UDP aggregated packet as needed. For example, an aggregated UDP GRO packet could land on a socket lacking the &lt;code&gt;UDP_GRO&lt;/code&gt; option due to some complex forwarding configuration. Segmenting the aggregated packet avoids confusing the receiver application with unexpected large datagrams. System-wide UDP GRO was implemented upstream with Linux 5.12. This feature still requires some kind of opt-in: It is off by default, and the system administrator can enable it on a per-network device basis.&lt;/p&gt; &lt;p&gt;Shortly afterward, with version 5.14, Linux gained additional support for UDP over UDP-tunnel GRO, again an opt-in feature that a system admin could enable the same way as basic UDP GRO. All the pieces needed to leverage TSO/GRO end-to-end for UDP application were in place—almost: UDP applications still have to enable TSO on the transmit side explicitly.&lt;/p&gt; &lt;p&gt;What about the existing applications with no built-in TSO support? The complexity in the container virtual network setup can be of some help, for once. Packets generated inside the container have to traverse a veth pair. Virtual ethernet forwarding is usually a straightforward and fast operation, but it can also be configured to optionally trigger GRO. This is usually not needed for TCP packets because they reach the veth already aggregated. But few user-space applications aggregate UDP packets.&lt;/p&gt; &lt;p&gt;Before Linux 5.14, to enable GRO on veth devices, a system administrator was required to attach an &lt;a href="https://developers.redhat.com/blog/2018/12/06/achieving-high-performance-low-latency-networking-with-xdp-part-1"&gt;eXpress Data Path (XDP)&lt;/a&gt; program to the veth pair. Linux 5.14 removes that constraint: Instead, by exploiting the GRO stage in the veth pair, the container virtual networking software can transparently aggregate UDP packets and forward them in the aggregate form for most of the virtual network topology, greatly reducing the overhead of forwarding.&lt;/p&gt; &lt;h2&gt;Availability in Red Hat Enterprise Linux 8.5&lt;/h2&gt; &lt;p&gt;So far, we have talked about the upstream Linux kernel project, but production deployments rarely use an upstream kernel. It's reasonable to ask when you'll be able to use all the enhancements described in this article in your preferred distro of choice? Very soon: All the relevant patches and features will land in the upcoming Red Hat Enterprise Linux 8.5.&lt;/p&gt; &lt;p&gt;Sounds great—your containerized UDP application will run twice as fast. Or not, since you have to enable something and you don't know what the heck to do. The next release of &lt;a href="https://developers.redhat.com/openshift"&gt;OpenShift Container Platform (OCP)&lt;/a&gt; will enable this feature, so the system administrator won't have to bother with additional setup. But if you can't wait, let's look at the gory configuration details.&lt;/p&gt; &lt;h2&gt;Enabling UDP GRO hands-on&lt;/h2&gt; &lt;p&gt;To enable GRO for UDP in a typical container virtual network setup, the sysadmin must:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Enable GRO on the veth peer in the main network namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;VETH=&lt;veth device name&gt; CPUS=`/usr/bin/nproc` ethtool -K $VETH gro on ethtool -L $VETH rx $CPUS tx $CPUS echo 50000 &gt; /sys/class/net/$VETH/gro_flush_timeout &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Enable GRO forwarding on that device: &lt;pre&gt; &lt;code class="language-bash"&gt;ethtool -K $VETH rx-udp-gro-forwarding on &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Enable GRO forwarding on the NIC connected to the wire: &lt;pre&gt; &lt;code class="language-bash"&gt;DEV=&lt;real NIC name&gt; ethtool -K $DEV rx-udp-gro-forwarding on &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The first step requires some additional explanation. The veth device performs the GRO activity on a per receive queue basis. The &lt;code&gt;ethtool -L&lt;/code&gt; command configures the number of active veth receive queues to match the number of CPUs on the host. That configuration prevents contention while scheduling and running GRO.&lt;/p&gt; &lt;p&gt;To allow the GRO engine to aggregate multiple packets, the veth needs to stage the eligible packets into the GRO engine. Traditional NICs implement this strategy in hardware on top of interrupt requests, but veth devices don't provide such a capability. Instead, with the provided settings, when a packet is staged into the GRO engine, the kernel sets a software timeout. When the timeout expires, the GRO engine is flushed. The &lt;code&gt;echo&lt;/code&gt; command in step 1 configures the timeout in nanoseconds. The higher the timeout, the higher the chance that multiple packets will be aggregated—but at the expense of introducing a higher latency.&lt;/p&gt; &lt;p&gt;This example sets the timeout to 50 microseconds, which is high enough to aggregate a reasonable number of UDP packets under a significant load while keeping the added latency acceptably low for most applications. This value should likely work for all except strict, hard real-time environments.&lt;/p&gt; &lt;p&gt;After you issue the preceding commands, any containerized application using UDP transparently benefits from GRO forwarding.&lt;/p&gt; &lt;h2&gt;Benchmarking GRO&lt;/h2&gt; &lt;p&gt;We measured the gain of GRO by running the &lt;code&gt;iperf&lt;/code&gt; tool at both ends of a connection in different compute nodes. Using a pair of nodes connected with a 10Gbps Ethernet link and a packet size equal to the allowed maximum transmission unit (MTU), we got the results shown in Figure 2. GRO showed a notable improvement in speed until we reached 16 or more concurrent flows, at which point even the vanilla kernel was able to reach the link speed limit. Even when the speeds were the same, CPU utilization with GRO forwarding was almost halved.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="A bar chart depicting UDP GRO forwarding performances." data-entity-type="file" data-entity-uuid="91f44e60-11b2-4440-8f8b-124892499368" src="https://developers.redhat.com/sites/default/files/inline-images/vanilla_UDP%20GRO%20forwarding%20enabled_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2. UDP packet flows are much faster on kernels with UDP TSO/GRO, although the advantage decreases as the number of flows increases.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Successive Linux kernels have greatly reduced the penalty of using UDP, an improvement especially noticeable in containers. We recommend that you adopt Red Hat Enterprise Linux 8.5 or another distribution with an up-to-date Linux kernel and enable UDP TSO/GRO globally on your systems.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/05/improve-udp-performance-rhel-85" title="Improve UDP performance in RHEL 8.5"&gt;Improve UDP performance in RHEL 8.5&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/STb7ANGMifM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Paolo Abeni</dc:creator><dc:date>2021-11-05T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/11/05/improve-udp-performance-rhel-85</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 3.9.10 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GgXg3lUZoVc/eclipse-vert-x-3-9-10" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-3-9-10</id><updated>2021-11-05T00:00:00Z</updated><content type="html">Eclipse Vert.x version 3.9.10 has just been released. It fixes quite a few bugs that have been reported by the community.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GgXg3lUZoVc" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-3-9-10</feedburner:origLink></entry><entry><title type="html">RESTEasy 5.0.0 Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Kp12RgHHf-c/" /><author><name /></author><id>https://resteasy.github.io/2021/11/04/resteasy-5.0.0-release/</id><updated>2021-11-04T18:11:11Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Kp12RgHHf-c" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/11/04/resteasy-5.0.0-release/</feedburner:origLink></entry><entry><title type="html">How to debug WildFly security issues</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Jh1CfYWONU4/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-security/how-to-debug-wildfly-security-issues/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-debug-wildfly-security-issues</id><updated>2021-11-04T17:22:37Z</updated><content type="html">In this article we will discuss how to troubleshoot WildFly security issues by enabling the right Loggers or System Properties. WildFly security framework is based on Elytron. Before WildFly 25, you could still use Picketbox legacy framework. This is however discuss in the second part of this article. To debug WildFly security issues the main ... The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Jh1CfYWONU4" height="1" width="1" alt=""/&gt;</content><dc:creator>F.Marchioni</dc:creator><feedburner:origLink>http://www.mastertheboss.com/jbossas/jboss-security/how-to-debug-wildfly-security-issues/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-debug-wildfly-security-issues</feedburner:origLink></entry><entry><title type="html">The Road Towards a Kogito Public API</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/AVOXDniXQbQ/the-road-towards-a-kogito-public-api.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2021/11/the-road-towards-a-kogito-public-api.html</id><updated>2021-11-04T14:50:00Z</updated><content type="html">In the we showed how to use our fancy new Public API, but I threatened promised you we would get back to the design rationale. So, here we are. This blog post is rather long, so I decided to break down into two parts. It is loosely based on the presentation I gave during , but I am adding some details about DataContexts that I did not include during that live stream.    However, if you would rather sit though me blabbering about API design, please . Otherwise read on. SEVEN RED LINES A long, long time ago, Tiago Dolphine and I were tasked with the goal of designing a public API for our platform. The requirements were easy enough; the API should have been: * consistent across all components * extensible with new components while retaining consistency * flexible in the way components can define their own methods, because no component is totally equal to the others My immediate reaction was along the lines of: I don’t know how familiar you are with that video where the expert in red-line-drawing pictured above is being asked to draw . But that is kind of like how the requirements above felt at first: that is, very contrasting with each other. Moreover, ourselves, we wanted to go further and design something that scaled and allowed for multiple types of interactions. In fact, KIE 7 APIs are designed for a synchronous, Java-based, local workflow. We wanted the new design to scale even in an asynchronous, , setting. But in order do that, the API surface should have been minimal and account for communication over the wire. AN EMERGING DESIGN As we recovered from our confusion, we decided that the best strategy to deal with this was to first observe whatever we did so far, and look for patterns. Then evolve those patterns to account for the new use cases that we had in mind. After all, the code-generation procedure that in Kogito automatically materializes REST endpoints from business assets uses an internal Java API that evolved from our internal requirements. This API was not completely designed, rather for a large part it just evolved from emerging requirements. The reason why we never called it public is that we wanted to take our time to make it cohesive and consistent. That time had finally come. We started by observing how REST endpoints created and interacted with sessions, process instances, DMN models, PMML models. // Processes application. .get(Processes.class) .processById(processId) .createInstance(...) .start() // Rule Units application. .get(RuleUnits.class) .create(Class&lt;?&gt;) .createInstance(...) .fire() // PMML application. .get(PredictionModels.class) .getPredictionModel(modelName) .evaluateAll(...) ... // DMN application. .get(DecisionModels.class) .getDecisionModel(modelName) .evaluateAll(...) ... As you can see, all these API look different, but not wildly different. So a common pattern was emerging. In general we can see how the API is concerned with getting to a named resource, and then invoking a method over it. app.get(ModelType.class) .getResourceById($identifier) .action(values) In some cases, before you get to invoke the action() you may need to navigate further to a sub-component within that resource, e.g.: app.get(ModelType.class) .getResourceById($identifier) .instances() .getInstanceId($instance_id) .action(values) or: app.get(ModelType.class) .getResourceById($identifier) .subComponent() .getSubComponentId($instance_id) .action(values) for instance: application. .get(Processes.class) .processById(processId) .getInstance(instanceId) .getWorkItem(name) .update(values) The method is usually is a “verb” indicating an action to apply over the resource, with a payload that represents the data context over which the action should be applied OBSERVATIONS All these method calls have something in common. The breakthrough came when we came to the following 3 realizations: 1. The fluent API denotes a path 2. “Verbs” are always at the end of the method chain 3. Each method is applied to at most one “payload” Let’s see them in detail. OBSERVATION 1: THE FLUENT API DENOTES A PATH Imagine that we want to invoke a method (a “command”) over a process, or a process instance; or over a decision model, or over a prediction model; or over a rule unit. In order to identify these resources, we always have to pass an identifier. The identifier is constructed differently for each type of resource, and usually it is not self-contained; i.e. by the identifier alone, it is hard to predict what kind of resource it would point to; for instance, is my.resource the name of a process? Is it the name of a PMML? A DMN maybe? If you were sent this single string over the wire, how would you decode it? Well, you could create a structured object, instead: class ProcessInstanceId { ProcessId parentProcess; String instanceName; } class ProcessId { String processName; } But then, would you represent the type of the object? How would normalize and serialize this into a String? How can you make sure that this String is representable as an object not just in Java, but making sure that is consumable by a client, written, say, in Python? And here comes the first observation. Consider, for instance: app.get(Processes.class) .get($process_id) .instances() .get($instance_id) /processes/$process_id/instances/$instance_id This method invocation returns an object of type Process&lt;?&gt; over which you will invoke a command, for instance start(variables). Each method call may fail, because $process_id may not exist, it may not have child instances, or the instance $instance_id does not belong to that process. This eager evaluation of the method chain makes it hard to design an asynchronous API around it. In a trivial remote implementation, each method invocation would correspond to a remote call. However, if we look at the method chain, we notice that each invocation does not really perform an action, but rather retrieves an object, that retrieves another object, that retrieves yet another object… until the last item on the chain is reached, which is usually an action (a “verb”) like start() or evaluate().  This means that everything that comes before the action is really a path. And a Path is trivially serializable as a string: /processes/$process_id/instances/$instance_id &gt; This is the reason we introduced the AppRoot object, which provides a fluent, &gt; navigable, typed API to a resource, while still retaining the ability to &gt; convert into a string, and parseable back into a typed object.  The validation of such a path can be delayed until the entire path has been formed, and delegated to the action itself. Which brings us to the second observation OBSERVATION 2: “VERBS” ARE ALWAYS AT THE END OF THE METHOD CHAIN If the fluent API denotes a Path, we can delay validation and defer it when a command is invoked, . For example, if we don’t check if a process instance identifier exists until we try to start() it, then the start() command may perform that check.. app.get(Processes.class) .get($process_id) .instances() .get($instance_id) .start() In other words, “real” commands are always at the end of a chain of method calls; the method chain only serves the purpose of “finding” the right object instance. In fact, the relation between “commands” and the method chain that precedes them may become clearer if we rewrite it as follows: abort(app.get(Processes.class) .get($process_id) .instances() .get($instance_id)) or even: ProcessInstance pi = app.get(Processes.class) .get($process_id) .instances() .get($instance_id); start(pi, variables) This apparently small realization has a deep effect: it means that we can separate the definition of the commands from the way we address a process or a process instance. * It means that we can identify commands by their name * It also means that we may have multiple implementation of such commands In other words, the following text conveys about the same amount of information as the method chain above: START /processes/$process_id/instances/$instance_id variables And this makes it representable using pretty much any programming language. The text above can be seen as a simple specification of one of the “commands” that our system accepts. It includes information about 1. /processes the type of resource 2. /$process_id the identifier of that resource 3. /instances a type sub-component of that resource 4. /$instance_id the identifier of that sub-component  a collection of such rows  COMMAND /TYPE/$MAIN_ID/$SUB-PATH PAYLOAD forms the description of a service. For instance a service for a DMN engine may be: EVALUATE /decisions/$decision_id evaluation-context It should be easy to see how this easily map to method calls, REST service requests, or even messages.  &gt; In general, this is the reason why we introduced services; i.e., interfaces &gt; that describe the entire API surface of an engine, and multiple interfaces and &gt; implementations can be provided for the same engine (for instance a &gt; synchronous vs an asynchronous process service). For instance: interface DecisionService { DataContext evaluate(Id identifier, DataContext ctx); } but also, possibly: interface AsyncDecisionService { Future&lt;DataContext&gt; evaluate(Id identifier, DataContext ctx); } In the next part of this post we will see this principle more in detail. OBSERVATION 3: EACH METHOD IS APPLIED TO AT MOST ONE PAYLOAD (AND POSSIBLY EXTRA METADATA) Each method is generally applied to an object that sometimes is called a model or variables (processes), sometimes data (RuleUnitData, for rules), sometimes context (DMN and PMML). More generally that is the payload of the request   The object is generally used to extract values or to send values through a command. application. .get(Processes.class) .processById(processId) .getInstanceById(instanceId) .updateVariables(model) Now, a frequent pattern for the model object is to convert that object into another type. For instance, it is frequent to project variables of a project to show only “input” variables and “output” variables. In other cases, it may be useful to extract the computation results as a Map and then convert that Map to another type. In any case, if we consider a distributed, polyglot execution environment, tying the representation of the data that is manipulated through an engine feels very restrictive. &gt; This is why we introduced the DataContext interface, shared across &gt; implementations, and requiring the simple contract that a DataContext should &gt; be convertible into another type of DataContext through some mapping &gt; mechanism. CONCLUSION In this post, I gave an overview of the principles and reasoning that were put into the design of the new public API. In the next post, I will show what these principles may enable in the future. In particular we will see how using paths for identifiers makes it easier to adopt it in multiple situations (such as data binding), that in turn enables a lot of convenient use cases. We will also see how the simple service-based API definition scheme may be useful in the future to enable service discovery and distributed execution.  The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/AVOXDniXQbQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Edoardo Vacchi</dc:creator><feedburner:origLink>https://blog.kie.org/2021/11/the-road-towards-a-kogito-public-api.html</feedburner:origLink></entry><entry><title>Generating pseudorandom numbers in Python</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/q9fyP6mJ738/generating-pseudorandom-numbers-python" /><author><name>Fridolin Pokorny</name></author><id>53b16b88-c242-4380-a53b-b3132232368b</id><updated>2021-11-04T07:00:00Z</updated><published>2021-11-04T07:00:00Z</published><summary type="html">&lt;p&gt;Random functions typically assign the same priority to each possible choice. In some cases, though, you want to be able to make a random choice while prioritizing some options. For instance, in &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;, we need to prioritize more recent releases of &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; packages. We use pseudorandom number calculation to prioritize newer libraries in the exploration phase of Thoth's &lt;a href="https://thoth-station.ninja/docs/developers/adviser/predictors/reinforcement_learning.html"&gt;reinforcement learning algorithm&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This article explores &lt;em&gt;termial random&lt;/em&gt;, a specific type of pseudorandom number calculation used in Project Thoth. We'll use the termial-random number generator to select an item from a list, assign the highest probability to the item at index 0, then assign lower probabilities to the following items as the index increases. You can apply the discussion and resources in this article to other Python projects.&lt;/p&gt; &lt;h2&gt;Pseudorandom number generation in Python&lt;/h2&gt; &lt;p&gt;The Python standard library offers &lt;a href="https://machinelearningmastery.com/how-to-generate-random-numbers-in-python/"&gt;several functions&lt;/a&gt; for pseudorandom number generation. For instance, if we want to pick an item randomly from a list, the &lt;code&gt;random.choice&lt;/code&gt; method works well:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;import random my_list = [42, 33, 30, 16] # results in 42 with a probability of 1 / len(my_list) random.choice(my_list)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let’s say we want to give higher numbers a higher probability of being chosen. In other words, in the &lt;code&gt;my_list&lt;/code&gt; example, we want to prioritize 42 over 33, 33 over 30, and 30 over 16.&lt;/p&gt; &lt;h2&gt;Weighted random choice in Python&lt;/h2&gt; &lt;p&gt;We have four numbers in total in our list, so let’s assign weights to these numbers as shown in Table 1.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="50%"&gt;&lt;caption&gt;Table 1. Weights assigned to numbers.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="col"&gt;Number&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Weight&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;td class="text-align-center"&gt;4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p class="text-align-center"&gt;33&lt;/p&gt; &lt;/td&gt; &lt;td class="text-align-center"&gt;3&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;30&lt;/td&gt; &lt;td class="text-align-center"&gt;2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;16&lt;/td&gt; &lt;td class="text-align-center"&gt;1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;You can think of each weight as a number of "buckets" assigned to the number. In a randomly uniform way, our algorithm tries to hit one bucket. After hitting the bucket, we check which number the bucket corresponds to.&lt;/p&gt; &lt;p&gt;The total number of buckets we can hit is equal to the sum of the weights:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;4 + 3 + 2 + 1 = 10&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Table 2 shows the probability of hitting each number, based on the buckets assigned to it, where all probabilities total up to 1.0.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="50%"&gt;&lt;caption&gt;Table 2. The probability of hitting a number.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="col"&gt;Number&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Probability&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;td class="text-align-center"&gt;4 / 10 = 0.4&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;33&lt;/td&gt; &lt;td&gt; &lt;p class="text-align-center"&gt;3 / 10 = 0.3&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;30&lt;/td&gt; &lt;td class="text-align-center"&gt;2 / 10 = 0.2&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;16&lt;/td&gt; &lt;td class="text-align-center"&gt;1 / 10 = 0.1&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Termial random number calculation&lt;/h2&gt; &lt;p&gt;To generalize this prioritization for &lt;em&gt;n&lt;/em&gt; numbers, we can create the following formula that computes the total number of buckets to use for any &lt;em&gt;n&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;n? = 1 + 2 + 3 + ... + (n - 2) + (n - 1) + n&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We could also write this formula as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/termial.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/termial.png?itok=JKYI3HdO" width="250" height="149" alt="Another way to write the termial formula. " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Another way to write the termial formula. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The formula is called a &lt;em&gt;termial&lt;/em&gt; as an analogy to factorials. The concept is related to &lt;a href="https://en.wikipedia.org/wiki/Triangular_number"&gt;triangular numbers&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Computing the termial of &lt;em&gt;n&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;To compute the termial of &lt;em&gt;n&lt;/em&gt; in Python, the simplest implementation is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;termial_of_n = sum(range(1, len(my_list) + 1)) # O(N)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A more efficient calculation uses the &lt;a href="https://en.wikipedia.org/wiki/Binomial_coefficient"&gt;binomial coefficient&lt;/a&gt; and computes &lt;code&gt;(len(my_list) + 1)&lt;/code&gt; over &lt;code&gt;2&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;l = len(my_list) # (l + 1) over 2 = l! / (2!*(l-2)!) = l * (l - 1) / 2 termial_of_n = ((l*l) + l) &gt;&gt; 1 # O(1)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, we can pick a random (random uniform) bucket from our set of buckets:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;import random choice = random.randrange(termial_of_n)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The result, stored in the variable &lt;code&gt;choice&lt;/code&gt;, holds an integer from 0 to 9 (inclusively) and represents an index into the list of the buckets we created earlier, as shown in Table 3.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="50%"&gt;&lt;caption&gt;Table 3. A complete list of buckets and possible choices.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th class="text-align-center" scope="col"&gt;Choice&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Bucket&lt;/th&gt; &lt;th class="text-align-center" scope="col"&gt;Number&lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;0&lt;/td&gt; &lt;td class="text-align-center"&gt;1&lt;/td&gt; &lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;1&lt;/td&gt; &lt;td class="text-align-center"&gt;2&lt;/td&gt; &lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;2&lt;/td&gt; &lt;td class="text-align-center"&gt;3&lt;/td&gt; &lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;3&lt;/td&gt; &lt;td class="text-align-center"&gt;4&lt;/td&gt; &lt;td class="text-align-center"&gt;42&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;4&lt;/td&gt; &lt;td class="text-align-center"&gt;5&lt;/td&gt; &lt;td class="text-align-center"&gt;33&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;5&lt;/td&gt; &lt;td class="text-align-center"&gt;6&lt;/td&gt; &lt;td class="text-align-center"&gt;33&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;6&lt;/td&gt; &lt;td class="text-align-center"&gt;7&lt;/td&gt; &lt;td class="text-align-center"&gt;33&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;7&lt;/td&gt; &lt;td class="text-align-center"&gt;8&lt;/td&gt; &lt;td class="text-align-center"&gt;30&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;8&lt;/td&gt; &lt;td class="text-align-center"&gt;9&lt;/td&gt; &lt;td class="text-align-center"&gt;30&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td class="text-align-center"&gt;9&lt;/td&gt; &lt;td class="text-align-center"&gt;10&lt;/td&gt; &lt;td class="text-align-center"&gt;16&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h3&gt;Termial random with the binomial coefficient&lt;/h3&gt; &lt;p&gt;Now, how do we find which number we hit through a randomly picked bucket for any &lt;em&gt;n&lt;/em&gt;? Let’s revisit how we computed the termial number of &lt;em&gt;n&lt;/em&gt; using the formula based on the binomial coefficient:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;l = len(my_list) termial_of_n = ((l*l) + l) &gt;&gt; 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Following the definition of the termial function, we know that regardless of &lt;em&gt;n&lt;/em&gt;, we always assign one bucket to the number at index &lt;em&gt;n-1&lt;/em&gt;, two buckets to the number at index &lt;em&gt;n-2&lt;/em&gt;, three buckets to the number at index &lt;em&gt;n-3&lt;/em&gt;, and so on, down to the index 0. Using this knowledge, we can transform the binomial coefficient formula to the following equation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;choice = ((i*i) + i) &gt;&gt; 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next step is to find &lt;code&gt;i&lt;/code&gt; that satisfies the given equation. The equation is a quadratic function described as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;a*(i**2) + b*i + c = 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The values of our coefficients are:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;a = 1/2 b = 1/2 c = -choice&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Because &lt;code&gt;choice&lt;/code&gt; is expected always to be a non-negative integer (an index into the list of buckets), we can search for a solution that always results in a non-negative integer (reducing one discriminant term that always results in negative &lt;code&gt;i&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;import math # D = b**2 - 4*a*c # x1 = (-b + math.sqrt(D)) / (2*a) # x2 = (-b - math.sqrt(D)) / (2*a) # Given: # a = 1/2 # b = 1/2 # c = -choice # D = (1/2)**2 + 4*0.5*choice = 0.25 + 2*choice i = math.floor(-0.5 + math.sqrt(0.25 + (choice &lt;&lt; 1)))&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The solution has to be rounded using &lt;code&gt;math.floor&lt;/code&gt; because it corresponds to the inverted index with respect to &lt;em&gt;n&lt;/em&gt;. Because &lt;code&gt;i&lt;/code&gt; is inverted, the final solution (index to the original list) is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;my_list[n - 1 - i]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Running the termial-random number generator&lt;/h2&gt; &lt;p&gt;Now, let's do the asymptotic complexity analysis, assuming that:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;len&lt;/code&gt; function can return the length of the list in &lt;em&gt;O(1)&lt;/em&gt; time.&lt;/li&gt; &lt;li&gt;&lt;code&gt;random.randrange&lt;/code&gt; operates in &lt;em&gt;O(1)&lt;/em&gt; time.&lt;/li&gt; &lt;li&gt;We use the equation based on the binomial coefficient to compute the termial of &lt;em&gt;n&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The whole computation is done in &lt;em&gt;O(1)&lt;/em&gt; time and &lt;em&gt;O(1)&lt;/em&gt; space.&lt;/p&gt; &lt;p&gt;If we used the sum-based computation of the termial of &lt;em&gt;n&lt;/em&gt;, the algorithm would require &lt;em&gt;O(n)&lt;/em&gt; time and &lt;em&gt;O(1)&lt;/em&gt; space.&lt;/p&gt; &lt;p&gt;The final source code in Python is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;import random import math def random_termial(n: int) -&gt; int: termial_of_n = ((n * n) + n) &gt;&gt; 1 choice = random.randrange(termial_of_n) i = math.floor(-0.5 + math.sqrt(0.25 + (choice &lt;&lt; 1))) return n - 1 - i&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 2 shows the number of hits for &lt;em&gt;n&lt;/em&gt;=10 when the termial random generator was run one million times:&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/chart.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/chart.png?itok=j_AJwJdT" width="640" height="480" alt="A benchmark shows that the termial generator works, choosing high-priority items more often than low-priority items." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A benchmark with the termial random number generator. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The chart shows that, just as we want, index 0 is prioritized most of the time; after that, index 1 is prioritized, and so on. The lowest priority is given to the index 9.&lt;/p&gt; &lt;h2&gt;Where to find the termial-random package&lt;/h2&gt; &lt;p&gt;The Project Thoth recommendation engine is available in a component called &lt;a href="https://github.com/thoth-station/adviser"&gt;adviser&lt;/a&gt; and uses a C extension that implements the termial random calculation. The C extension is available on PyPI as the &lt;a href="https://pypi.org/project/termial-random/"&gt;termial-random package&lt;/a&gt; and the source code is hosted in the &lt;a href="https://github.com/thoth-station/termial-random"&gt;thoth-station/termial-random&lt;/a&gt; repository.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow updates in &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;, feel free to &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;subscribe to our YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation Twitter handle&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/04/generating-pseudorandom-numbers-python" title="Generating pseudorandom numbers in Python "&gt;Generating pseudorandom numbers in Python &lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/q9fyP6mJ738" height="1" width="1" alt=""/&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2021-11-04T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/11/04/generating-pseudorandom-numbers-python</feedburner:origLink></entry><entry><title type="html">WildFly 25.0.1 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yoMsomhQu3E/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2021/11/04/WildFly2501-Released/</id><updated>2021-11-04T00:00:00Z</updated><content type="html">WildFly 25.0.1.Final is now available . It’s been about four weeks since the WildFly 25 release, so we’ve done a small bug fix update, WildFly 25.0.1. This includes an update to WildFly Preview. The full list of issues resolved in WildFly 25.0.1 is available . Issues resolved in the WildFly Core updates (there were two) included with WildFly 25.0.1 are available and . Enjoy!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yoMsomhQu3E" height="1" width="1" alt=""/&gt;</content><dc:creator>Kabir Khan</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/11/04/WildFly2501-Released/</feedburner:origLink></entry><entry><title>Red Hat Enterprise Linux 9 Beta is here</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ZgETOirB8RU/red-hat-enterprise-linux-9-beta-here" /><author><name>Don Pinto</name></author><id>588923a3-91f4-4ccd-a5a0-e6ab3c4059d3</id><updated>2021-11-03T09:07:03Z</updated><published>2021-11-03T09:07:03Z</published><summary type="html">&lt;p&gt;We’ve been working hard on &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; 9. Red Hat Enterprise Linux 9 Beta is now available—and it’s been built with production stability and development agility in mind. Built from &lt;a href="https://www.redhat.com/en/topics/linux/what-is-centos-stream"&gt;CentOS Stream&lt;/a&gt;, RHEL 9 Beta delivers an easier application development experience based on a new platform with powerful capabilities.&lt;/p&gt; &lt;p&gt;Here are five key highlights for developers:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Modernize your applications using the latest versions of &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;GCC&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;LLVM&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;Rust&lt;/a&gt; compilers.&lt;/li&gt; &lt;li aria-level="1"&gt;Get 10+ years of enterprise-class platform stability with version 2.34 of the GNU C Library project (glibc).&lt;/li&gt; &lt;li aria-level="1"&gt;Power your &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; applications using Python 3.9.&lt;/li&gt; &lt;li aria-level="1"&gt;Access different software versions easily through enhanced application stream packaging options.&lt;/li&gt; &lt;li aria-level="1"&gt;Benefit from &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; ecosystem support, thanks to CentOS Stream.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now, let’s dive into the details a bit more to explain what all of this means.&lt;/p&gt; &lt;h2&gt;Compiler updates and new features&lt;/h2&gt; &lt;p&gt;A new major version of an operating system brings new opportunities—especially when it comes to application modernization. Red Hat Enterprise Linux 9 includes GCC 11 as well as the latest versions of Go, LLVM, and Rust compilers, which lets you modernize your applications to benefit from the latest compiler optimizations and features.&lt;/p&gt; &lt;h3&gt;GCC 11&lt;/h3&gt; &lt;p&gt;Build powerful &lt;a href="https://developers.redhat.com/topics/c/"&gt;C/C++&lt;/a&gt; applications for Red Hat Enterprise Linux 9 with the new GCC 11 system compiler. This release makes C++ 17 the default standard for the C++ language, instead of the previously supported C++ 14 standard. This version includes a &lt;a href="https://developers.redhat.com/blog/2021/01/28/static-analysis-updates-in-gcc-11#"&gt;more robust static analysis&lt;/a&gt; option (&lt;code&gt;-fanalyzer&lt;/code&gt;), and a host of other new C++ 17 features.&lt;/p&gt; &lt;p&gt;However, the new C++ 17 language standard also deprecates, removes, or changes the semantics of certain constructs. Read &lt;a href="https://developers.redhat.com/articles/2021/08/06/porting-your-code-c17-gcc-11"&gt;Porting your code to C++17 with GCC 11&lt;/a&gt; to learn more about how you can modernize your application to C++17 with GCC 11.&lt;/p&gt; &lt;h3&gt;Link Time Optimization&lt;/h3&gt; &lt;p&gt;Link Time Optimization (LTO) is enabled in Red Hat Enterprise Linux 9, and the system comes with a number of packages that are built with it. As a result, applications have smaller and faster binaries and allow deeper inspection of source code at compile time. This can improve GCC diagnostics for potential coding errors, such as One Definition Rule (ODR) violations.&lt;/p&gt; &lt;h3&gt;Go 1.16, LLVM 12, and Rust 1.54&lt;/h3&gt; &lt;p&gt;Go 1.16 brings support for the new embed package, enabling developers to bundle supporting data files into their Go programs and simplify developing with Go. With Go 1.16, modules are enabled by default making language dependencies easier to manage. Additionally, there are also several other improvements and performance optimizations.&lt;/p&gt; &lt;p&gt;With the latest LLVM 12 toolset, developers can take advantage of fresher tooling, and compatibility with other code built with compatible versions of LLVM/Clang.&lt;/p&gt; &lt;p&gt;Rust 1.54 lets developers write high-performance applications that run with a low memory footprint, making it highly suitable for edge use cases. Additionally, Rust is a statically typed language, making it easy to catch compile-time errors and maintain.&lt;/p&gt; &lt;h2&gt;glibc 2.34&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9 leverages GNU C Library Project (glibc) 2.34, enabling Red Hat Enterprise Linux to stay up to date with the latest security and bug fixes from glibc upstream. A few key benefits are improved performance, enhanced compliance with POSIX.1-2017, and additional locales.&lt;/p&gt; &lt;h2&gt;Python 3.9&lt;/h2&gt; &lt;p&gt;Python 3.9 is the system version of Python in Red Hat Enterprise Linux 9. This means that Python 3 is installed by default, and there are &lt;a href="https://developers.redhat.com/blog/2019/10/25/python-2-support-is-going-away-soon-make-the-move-to-python-3"&gt;no Python 2 packages available&lt;/a&gt;. Python 3.9 brings several new features to help developers modernize their applications, including timezone-aware timestamps, the new string prefix and suffix methods, and dictionary union operations. &lt;a href="https://developers.redhat.com/blog/2014/02/18/migrate-to-python3-w-rhscl#make_porting_easier_with_red_hat_software_collections"&gt;Learn more&lt;/a&gt; about features in Python 3 and get helpful tips to port your code.&lt;/p&gt; &lt;h2&gt;Next-generation application streams&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 8 introduced application streams to bring greater flexibility to developers exploring different versions of software. For example, we added updated applications and frameworks, like &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; 10, 12, 14, and 16 and PostgreSQL 9.6, 10, 12, and 13. Different versions of the software are installable using different modules.&lt;/p&gt; &lt;p&gt;The 9.0 Beta release builds on the success of application streams and module packaging in Red Hat Enterprise Linux 8. All packaging methods are incorporated into application streams including modules, software collections (SCLs), Flatpaks, and traditional RPMs, making them much easier to use.&lt;/p&gt; &lt;h2&gt;Built from CentOS Stream&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9 is the first major release of Red Hat Enterprise Linux built from &lt;a href="https://www.redhat.com/en/topics/linux/what-is-centos-stream"&gt;CentOS Stream&lt;/a&gt;. All future Red Hat Enterprise Linux releases will be built from it. This enables developers to contribute to and test the code prior to future versions being released.&lt;/p&gt; &lt;h2&gt;Download Red Hat Enterprise Linux 9 Beta&lt;/h2&gt; &lt;p&gt;The best way to experience Red Hat Enterprise Linux 9 Beta is by trying it out yourself. Red Hat Enterprise Linux 9 Beta is available to everyone participating in the Red Hat Developer program.&lt;/p&gt; &lt;p&gt;Here are a few ways you can get access to the Red Hat Enterprise Linux 9 Beta release:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/rhel/download"&gt;Sign up for the Red Hat Enterprise Linux Developer program&lt;/a&gt; (it’s free and easy to do).&lt;/li&gt; &lt;li aria-level="1"&gt;If you’re a current Red Hat Enterprise Linux corporate customer, &lt;a href="https://access.redhat.com/products/red-hat-enterprise-linux/#getstarted"&gt;log into the Customer Portal&lt;/a&gt; to get started.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Additional resources&lt;/h2&gt; &lt;p&gt;In this article, we have only scratched the surface of what's included in the Red Hat Enterprise Linux 9 Beta. This release brings several new features in automation and management, security and compliance, container improvements, performance, and more. &lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Read more about these new capabilities in the Red Hat Enterprise Linux 9 Beta &lt;a href="https://www.redhat.com/en/blog/whats-new-rhel-90-beta"&gt;release announcement&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/rhel/rhel9-beta"&gt;Learn about how to get started with Red Hat Enterprise Linux 9 Beta&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Check out the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9-beta/html/9.0_release_notes/index"&gt;Red Hat Enterprise Linux 9 Beta product documentation&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/03/red-hat-enterprise-linux-9-beta-here" title="Red Hat Enterprise Linux 9 Beta is here"&gt;Red Hat Enterprise Linux 9 Beta is here&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ZgETOirB8RU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Pinto</dc:creator><dc:date>2021-11-03T09:07:03Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/11/03/red-hat-enterprise-linux-9-beta-here</feedburner:origLink></entry><entry><title>How to use service binding with RabbitMQ</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/8dBfRNkcdmM/how-use-service-binding-rabbitmq" /><author><name>Andy Sadler</name></author><id>2095353d-a5a4-4dc6-ba55-cb3eeef7f3ff</id><updated>2021-11-03T07:00:00Z</updated><published>2021-11-03T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; ecosystem has inconsistent ways to expose Secrets to applications in order to allow them to connect to services. Many service providers have their own bespoke methods of binding an application to their services, which can slow down development teams considerably.&lt;/p&gt; &lt;p&gt;The Service Binding Operator remedies this by managing the binding process. This article walks through a simple example of service binding in action using the &lt;a href="https://developers.redhat.com/topics/open-source/"&gt;open source&lt;/a&gt; RabbitMQ message broker.&lt;/p&gt; &lt;h2&gt;How Service Binding Operator manages the binding process&lt;/h2&gt; &lt;p&gt;When you request a binding, the Service Binding Operator looks at information stored within the custom resource and its corresponding custom resource definition. This information tells the Service Binding Operator the proper method for binding the application. The Service Binding Operator then projects the binding method into the application's container through environment variables or files mounted within the container.&lt;/p&gt; &lt;p&gt;To learn more about other features of the Service Binding Operator and its integration with other products, read our release announcement &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Announcing Service Binding Operator 1.0 GA&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;About the example&lt;/h2&gt; &lt;p&gt;Let's say you have two Kubernetes services, &lt;code&gt;producer&lt;/code&gt; and &lt;code&gt;consumer&lt;/code&gt;, that talk to a &lt;a href="https://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt; instance using the &lt;a href="https://www.amqp.org/about/what/"&gt;Advanced Message Queuing Protocol (AMQP)&lt;/a&gt;. The &lt;code&gt;producer&lt;/code&gt; periodically produces data that the &lt;code&gt;consumer&lt;/code&gt; reads and acts on. For the sake of this demonstration, the consumer's action is simply to print whatever it receives to the standard output (&lt;code&gt;stdout&lt;/code&gt;).&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;First, install the RabbitMQ Operator:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl apply -f https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, install the &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt;, a prerequisite for the Service Binding Operator:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.19.1/install.sh | bash -s v0.19.1 &lt;/code&gt;&lt;/pre&gt; &lt;div class="Indent1"&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Yes, running &lt;code&gt;curl ... | bash&lt;/code&gt; isn't the best security. If this is a concern for you, save the installation script to a location in your filesystem and execute the script there after inspecting its contents.&lt;/p&gt; &lt;br /&gt;&lt;/div&gt; &lt;p&gt;Finally, you'll also need to install the Service Binding Operator itself:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl apply -f https://operatorhub.io/install/service-binding-operator.yaml &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the application on Kubernetes&lt;/h2&gt; &lt;p&gt;Next, you'll want to have the &lt;code&gt;producer&lt;/code&gt; and &lt;code&gt;consumer&lt;/code&gt; running on the Kubernetes cluster. For convenience, I've authored two containers that provide this functionality; their sources can be found in &lt;a href="https://github.com/sadlerap/sbo-rabbitmq-sample"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Service Binding Operator can operate against deployments, and deployments make the most sense for running the applications in this example. You can deploy them with the following commands:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl create deployment producer --image=quay.io/ansadler/rabbitmq-producer:1.0 $ kubectl create deployment consumer --image=quay.io/ansadler/rabbitmq-consumer:1.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll also need a RabbitMQ cluster to run them against:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;apiVersion: rabbitmq.com/v1beta1 kind: RabbitmqCluster metadata: name: rabbitmq spec: service: type: ClusterIP &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, if you inspect the container logs for the &lt;code&gt;consumer&lt;/code&gt; (which monitors the consumer process's &lt;code&gt;stdout&lt;/code&gt;), you'll see something similar to this:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl logs consumer-deployment-f877cffb6-p9sks Error: 0: $RABBITMQCLUSTER_HOST not defined Location: src/consumer.rs:16 Backtrace omitted. Run with RUST_BACKTRACE=1 environment variable to display it. Run with RUST_BACKTRACE=full to include source snippets. &lt;/code&gt;&lt;/pre&gt; &lt;div class="Indent1"&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The pod whose logs you will be inspecting will not be the same as this example, since the name of the pod that runs our applications will be different every time the deployment is changed. You can retrieve the name of the pod using the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="language-bash"&gt;$ kubectl get pods --selector=app=consumer&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;p&gt;If you inspect the logs for the &lt;code&gt;producer&lt;/code&gt; as well, you'll see that it throws a similar error. This happens because you haven't bound your RabbitMQ cluster to the &lt;code&gt;producer&lt;/code&gt; and &lt;code&gt;consumer&lt;/code&gt;, so neither of them knows where to find it. Let's fix that.&lt;/p&gt; &lt;h2&gt;Bind the services together with ServiceBindings&lt;/h2&gt; &lt;p&gt;If you were not using the Service Binding Operator, you would need to tell both the &lt;code&gt;producer&lt;/code&gt; and the &lt;code&gt;consumer&lt;/code&gt; how to connect to the RabbitMQ instance. This would require distributing at least the following information to these applications:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The hostname of the RabbitMQ instance.&lt;/li&gt; &lt;li&gt;The port that the RabbitMQ instance is listening on.&lt;/li&gt; &lt;li&gt;Authentication credentials (such as username and password).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This in turn would require you to expose your secrets to your applications, either by having the applications directly fetch that information from Kubernetes's API or by projecting that information into your applications yourself. Both of these methods are rather intrusive toward the applications, and it stands to reason that the process could be automated. And that's where the Service Binding Operator comes in.&lt;/p&gt; &lt;p&gt;To bind your applications and services together, the Service Binding Operator introduces a new custom resource called &lt;code&gt;ServiceBinding&lt;/code&gt;, which represents the binding between an application and a service. In this particular example, the bindings for our &lt;code&gt;producer&lt;/code&gt; and &lt;code&gt;consumer&lt;/code&gt; applications look like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;--- apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: servicebinding-consumer spec: bindAsFiles: false services: - group: rabbitmq.com version: v1beta1 kind: RabbitmqCluster name: rabbitmq application: name: consumer-deployment version: v1 group: apps resource: deployments --- apiVersion: binding.operators.coreos.com/v1alpha1 kind: ServiceBinding metadata: name: servicebinding-producer spec: bindAsFiles: false services: - group: rabbitmq.com version: v1beta1 kind: RabbitmqCluster name: rabbitmq application: name: producer-deployment version: v1 group: apps resource: deployments --- &lt;/code&gt;&lt;/pre&gt; &lt;div class="Indent1"&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are running this against an Operator not already supported by the Service Binding Operator (see our &lt;a href="https://github.com/redhat-developer/service-binding-operator#known-bindable-operators"&gt;README&lt;/a&gt; for a list of supported Operators), you will to give Service Binding Operator permission to read from this service according to the rules of role-based access control (RBAC). You can read more about how to grant this permission that in our &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/exposing-binding-data/rbac-requirements.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;br /&gt;&lt;/div&gt; &lt;p&gt;Now, if you inspect the logs of your &lt;code&gt;consumer&lt;/code&gt; deployment, you'll see that &lt;code&gt;producer&lt;/code&gt; has been sending messages to it. You should see something similar to the following:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl logs consumer-deployment-6f48dbfb7d-5dsgd connecting to: amqp://default_user_7Jba_ZP7NKD-UjJK8AQ:HIhVZ4a_6Xm60Z7bmbEDADDpwr2e_tch@rabbitmq.default.svc:5672 Waiting for messages, press Ctrl-C to exit. ( 0) Received [hello, world!] ( 1) Received [hello, world!] ( 2) Received [hello, world!] ( 3) Received [hello, world!] ( 4) Received [hello, world!] ( 5) Received [hello, world!] ( 6) Received [hello, world!] ( 7) Received [hello, world!] ( 8) Received [hello, world!] ( 9) Received [hello, world!] ( 10) Received [hello, world!] ( 11) Received [hello, world!] ( 12) Received [hello, world!] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;producer&lt;/code&gt; says something similar:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ kubectl logs producer-deployment-6d8d55949d-8qd9c connecting to: amqp://default_user_7Jba_ZP7NKD-UjJK8AQ:HIhVZ4a_6Xm60Z7bmbEDADDpwr2e_tch@rabbitmq.default.svc:5672 sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello sending [hello, world!] to queue hello &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Resources&lt;/h2&gt; &lt;p&gt;To learn more about the Service Binding Operator, check out the following resources:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;Service Binding Operator on GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://redhat-developer.github.io/service-binding-operator/"&gt;Service Binding Operator documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sadlerap/sbo-rabbitmq-sample"&gt;Materials used in this article&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/11/03/how-use-service-binding-rabbitmq" title="How to use service binding with RabbitMQ"&gt;How to use service binding with RabbitMQ&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/8dBfRNkcdmM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Andy Sadler</dc:creator><dc:date>2021-11-03T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/11/03/how-use-service-binding-rabbitmq</feedburner:origLink></entry><entry><title type="html">Automatically generate BPMN/DMN SVG on VS Code</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ypnTev8MIeM/automatically-generate-bpmn-dmn-svg-on-vs-code.html" /><author><name>Eder Ignatowicz</name></author><id>https://blog.kie.org/2021/11/automatically-generate-bpmn-dmn-svg-on-vs-code.html</id><updated>2021-11-03T05:00:00Z</updated><content type="html">To provide better integration with the KIE server and Business Central, on the , we introduced a way to, on VS Code, automatically generate SVG on each save of your BPMN and DMN Diagram. Take a look at this feature in action: HOW TO CONFIGURE IT To auto-generate on VS Code the SVG on each save of your BPMN and DMN diagrams; you need to add two properties to your user and workspace settings (settings.json, the VS Code configuration file): "kogito.bpmn.runOnSave": "extension.kogito.silentlyGenerateSvgBpmn", "kogito.dmn.runOnSave": "extension.kogito.silentlyGenerateSvgDmn", To do that, open your user and workspace settings, use the following VS Code menu command: * On Windows/Linux – File &gt; Preferences &gt; Settings * On macOS – Code &gt; Preferences &gt; Settings From there, access menu Extension &gt; BPMN (or DMN), and click on ‘Edit in settings.json’: And finally, add the respective properties to the end of this file and save it: If you need any further questions, please let us know in the comment section! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ypnTev8MIeM" height="1" width="1" alt=""/&gt;</content><dc:creator>Eder Ignatowicz</dc:creator><feedburner:origLink>https://blog.kie.org/2021/11/automatically-generate-bpmn-dmn-svg-on-vs-code.html</feedburner:origLink></entry></feed>
